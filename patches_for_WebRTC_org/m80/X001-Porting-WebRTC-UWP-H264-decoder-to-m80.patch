From 45006e8e23a1589d8b2e84a3c7f4344ec27e3bcb Mon Sep 17 00:00:00 2001
From: Augusto Righetto <aurighet@microsoft.com>
Date: Tue, 7 Apr 2020 17:31:38 -0700
Subject: [PATCH] Porting WebRTC-UWP H264 decoder to m80

---
 modules/video_coding/BUILD.gn                 |   18 +-
 modules/video_coding/codecs/h264/DEPS         |    2 -
 .../codecs/h264/h264_color_space.cc           |  310 ++---
 .../codecs/h264/h264_color_space.h            |   14 +-
 .../codecs/h264/h264_decoder_impl.cc          | 1137 +++++++++++++----
 .../codecs/h264/h264_decoder_impl.h           |   93 +-
 .../codecs/h264/h264_encoder_impl.cc          |  956 +++++++-------
 .../codecs/h264/h264_encoder_impl.h           |   14 +-
 8 files changed, 1597 insertions(+), 947 deletions(-)

diff --git a/modules/video_coding/BUILD.gn b/modules/video_coding/BUILD.gn
index ceee019e06..cc8d3aa6ab 100644
--- a/modules/video_coding/BUILD.gn
+++ b/modules/video_coding/BUILD.gn
@@ -345,15 +345,15 @@ rtc_library("webrtc_h264") {
     "//third_party/libyuv",
   ]
 
-  if (rtc_use_h264) {
-    deps += [
-      "//third_party/ffmpeg",
-      "//third_party/openh264:encoder",
-    ]
-    if (!build_with_mozilla) {
-      deps += [ "../../media:rtc_media_base" ]
-    }
-  }
+#  if (rtc_use_h264) {
+#    deps += [
+#      "//third_party/ffmpeg",
+#      "//third_party/openh264:encoder",
+#    ]
+#    if (!build_with_mozilla) {
+#      deps += [ "../../media:rtc_media_base" ]
+#    }
+#  }
 }
 
 rtc_library("webrtc_multiplex") {
diff --git a/modules/video_coding/codecs/h264/DEPS b/modules/video_coding/codecs/h264/DEPS
index 4e110917d8..cc5cd70142 100644
--- a/modules/video_coding/codecs/h264/DEPS
+++ b/modules/video_coding/codecs/h264/DEPS
@@ -1,5 +1,3 @@
 include_rules = [
-  "+third_party/ffmpeg",
-  "+third_party/openh264",
   "+media/base",
 ]
diff --git a/modules/video_coding/codecs/h264/h264_color_space.cc b/modules/video_coding/codecs/h264/h264_color_space.cc
index 59921263e3..6636b174e3 100644
--- a/modules/video_coding/codecs/h264/h264_color_space.cc
+++ b/modules/video_coding/codecs/h264/h264_color_space.cc
@@ -17,161 +17,161 @@
 
 namespace webrtc {
 
-ColorSpace ExtractH264ColorSpace(AVCodecContext* codec) {
-  ColorSpace::PrimaryID primaries = ColorSpace::PrimaryID::kUnspecified;
-  switch (codec->color_primaries) {
-    case AVCOL_PRI_BT709:
-      primaries = ColorSpace::PrimaryID::kBT709;
-      break;
-    case AVCOL_PRI_BT470M:
-      primaries = ColorSpace::PrimaryID::kBT470M;
-      break;
-    case AVCOL_PRI_BT470BG:
-      primaries = ColorSpace::PrimaryID::kBT470BG;
-      break;
-    case AVCOL_PRI_SMPTE170M:
-      primaries = ColorSpace::PrimaryID::kSMPTE170M;
-      break;
-    case AVCOL_PRI_SMPTE240M:
-      primaries = ColorSpace::PrimaryID::kSMPTE240M;
-      break;
-    case AVCOL_PRI_FILM:
-      primaries = ColorSpace::PrimaryID::kFILM;
-      break;
-    case AVCOL_PRI_BT2020:
-      primaries = ColorSpace::PrimaryID::kBT2020;
-      break;
-    case AVCOL_PRI_SMPTE428:
-      primaries = ColorSpace::PrimaryID::kSMPTEST428;
-      break;
-    case AVCOL_PRI_SMPTE431:
-      primaries = ColorSpace::PrimaryID::kSMPTEST431;
-      break;
-    case AVCOL_PRI_SMPTE432:
-      primaries = ColorSpace::PrimaryID::kSMPTEST432;
-      break;
-    case AVCOL_PRI_JEDEC_P22:
-      primaries = ColorSpace::PrimaryID::kJEDECP22;
-      break;
-    case AVCOL_PRI_RESERVED0:
-    case AVCOL_PRI_UNSPECIFIED:
-    case AVCOL_PRI_RESERVED:
-    default:
-      break;
-  }
-
-  ColorSpace::TransferID transfer = ColorSpace::TransferID::kUnspecified;
-  switch (codec->color_trc) {
-    case AVCOL_TRC_BT709:
-      transfer = ColorSpace::TransferID::kBT709;
-      break;
-    case AVCOL_TRC_GAMMA22:
-      transfer = ColorSpace::TransferID::kGAMMA22;
-      break;
-    case AVCOL_TRC_GAMMA28:
-      transfer = ColorSpace::TransferID::kGAMMA28;
-      break;
-    case AVCOL_TRC_SMPTE170M:
-      transfer = ColorSpace::TransferID::kSMPTE170M;
-      break;
-    case AVCOL_TRC_SMPTE240M:
-      transfer = ColorSpace::TransferID::kSMPTE240M;
-      break;
-    case AVCOL_TRC_LINEAR:
-      transfer = ColorSpace::TransferID::kLINEAR;
-      break;
-    case AVCOL_TRC_LOG:
-      transfer = ColorSpace::TransferID::kLOG;
-      break;
-    case AVCOL_TRC_LOG_SQRT:
-      transfer = ColorSpace::TransferID::kLOG_SQRT;
-      break;
-    case AVCOL_TRC_IEC61966_2_4:
-      transfer = ColorSpace::TransferID::kIEC61966_2_4;
-      break;
-    case AVCOL_TRC_BT1361_ECG:
-      transfer = ColorSpace::TransferID::kBT1361_ECG;
-      break;
-    case AVCOL_TRC_IEC61966_2_1:
-      transfer = ColorSpace::TransferID::kIEC61966_2_1;
-      break;
-    case AVCOL_TRC_BT2020_10:
-      transfer = ColorSpace::TransferID::kBT2020_10;
-      break;
-    case AVCOL_TRC_BT2020_12:
-      transfer = ColorSpace::TransferID::kBT2020_12;
-      break;
-    case AVCOL_TRC_SMPTE2084:
-      transfer = ColorSpace::TransferID::kSMPTEST2084;
-      break;
-    case AVCOL_TRC_SMPTE428:
-      transfer = ColorSpace::TransferID::kSMPTEST428;
-      break;
-    case AVCOL_TRC_ARIB_STD_B67:
-      transfer = ColorSpace::TransferID::kARIB_STD_B67;
-      break;
-    case AVCOL_TRC_RESERVED0:
-    case AVCOL_TRC_UNSPECIFIED:
-    case AVCOL_TRC_RESERVED:
-    default:
-      break;
-  }
-
-  ColorSpace::MatrixID matrix = ColorSpace::MatrixID::kUnspecified;
-  switch (codec->colorspace) {
-    case AVCOL_SPC_RGB:
-      matrix = ColorSpace::MatrixID::kRGB;
-      break;
-    case AVCOL_SPC_BT709:
-      matrix = ColorSpace::MatrixID::kBT709;
-      break;
-    case AVCOL_SPC_FCC:
-      matrix = ColorSpace::MatrixID::kFCC;
-      break;
-    case AVCOL_SPC_BT470BG:
-      matrix = ColorSpace::MatrixID::kBT470BG;
-      break;
-    case AVCOL_SPC_SMPTE170M:
-      matrix = ColorSpace::MatrixID::kSMPTE170M;
-      break;
-    case AVCOL_SPC_SMPTE240M:
-      matrix = ColorSpace::MatrixID::kSMPTE240M;
-      break;
-    case AVCOL_SPC_YCGCO:
-      matrix = ColorSpace::MatrixID::kYCOCG;
-      break;
-    case AVCOL_SPC_BT2020_NCL:
-      matrix = ColorSpace::MatrixID::kBT2020_NCL;
-      break;
-    case AVCOL_SPC_BT2020_CL:
-      matrix = ColorSpace::MatrixID::kBT2020_CL;
-      break;
-    case AVCOL_SPC_SMPTE2085:
-      matrix = ColorSpace::MatrixID::kSMPTE2085;
-      break;
-    case AVCOL_SPC_CHROMA_DERIVED_NCL:
-    case AVCOL_SPC_CHROMA_DERIVED_CL:
-    case AVCOL_SPC_ICTCP:
-    case AVCOL_SPC_UNSPECIFIED:
-    case AVCOL_SPC_RESERVED:
-    default:
-      break;
-  }
-
-  ColorSpace::RangeID range = ColorSpace::RangeID::kInvalid;
-  switch (codec->color_range) {
-    case AVCOL_RANGE_MPEG:
-      range = ColorSpace::RangeID::kLimited;
-      break;
-    case AVCOL_RANGE_JPEG:
-      range = ColorSpace::RangeID::kFull;
-      break;
-    case AVCOL_RANGE_UNSPECIFIED:
-    default:
-      break;
-  }
-  return ColorSpace(primaries, transfer, matrix, range);
-}
+//ColorSpace ExtractH264ColorSpace(AVCodecContext* codec) {
+//  ColorSpace::PrimaryID primaries = ColorSpace::PrimaryID::kUnspecified;
+//  switch (codec->color_primaries) {
+//    case AVCOL_PRI_BT709:
+//      primaries = ColorSpace::PrimaryID::kBT709;
+//      break;
+//    case AVCOL_PRI_BT470M:
+//      primaries = ColorSpace::PrimaryID::kBT470M;
+//      break;
+//    case AVCOL_PRI_BT470BG:
+//      primaries = ColorSpace::PrimaryID::kBT470BG;
+//      break;
+//    case AVCOL_PRI_SMPTE170M:
+//      primaries = ColorSpace::PrimaryID::kSMPTE170M;
+//      break;
+//    case AVCOL_PRI_SMPTE240M:
+//      primaries = ColorSpace::PrimaryID::kSMPTE240M;
+//      break;
+//    case AVCOL_PRI_FILM:
+//      primaries = ColorSpace::PrimaryID::kFILM;
+//      break;
+//    case AVCOL_PRI_BT2020:
+//      primaries = ColorSpace::PrimaryID::kBT2020;
+//      break;
+//    case AVCOL_PRI_SMPTE428:
+//      primaries = ColorSpace::PrimaryID::kSMPTEST428;
+//      break;
+//    case AVCOL_PRI_SMPTE431:
+//      primaries = ColorSpace::PrimaryID::kSMPTEST431;
+//      break;
+//    case AVCOL_PRI_SMPTE432:
+//      primaries = ColorSpace::PrimaryID::kSMPTEST432;
+//      break;
+//    case AVCOL_PRI_JEDEC_P22:
+//      primaries = ColorSpace::PrimaryID::kJEDECP22;
+//      break;
+//    case AVCOL_PRI_RESERVED0:
+//    case AVCOL_PRI_UNSPECIFIED:
+//    case AVCOL_PRI_RESERVED:
+//    default:
+//      break;
+//  }
+//
+//  ColorSpace::TransferID transfer = ColorSpace::TransferID::kUnspecified;
+//  switch (codec->color_trc) {
+//    case AVCOL_TRC_BT709:
+//      transfer = ColorSpace::TransferID::kBT709;
+//      break;
+//    case AVCOL_TRC_GAMMA22:
+//      transfer = ColorSpace::TransferID::kGAMMA22;
+//      break;
+//    case AVCOL_TRC_GAMMA28:
+//      transfer = ColorSpace::TransferID::kGAMMA28;
+//      break;
+//    case AVCOL_TRC_SMPTE170M:
+//      transfer = ColorSpace::TransferID::kSMPTE170M;
+//      break;
+//    case AVCOL_TRC_SMPTE240M:
+//      transfer = ColorSpace::TransferID::kSMPTE240M;
+//      break;
+//    case AVCOL_TRC_LINEAR:
+//      transfer = ColorSpace::TransferID::kLINEAR;
+//      break;
+//    case AVCOL_TRC_LOG:
+//      transfer = ColorSpace::TransferID::kLOG;
+//      break;
+//    case AVCOL_TRC_LOG_SQRT:
+//      transfer = ColorSpace::TransferID::kLOG_SQRT;
+//      break;
+//    case AVCOL_TRC_IEC61966_2_4:
+//      transfer = ColorSpace::TransferID::kIEC61966_2_4;
+//      break;
+//    case AVCOL_TRC_BT1361_ECG:
+//      transfer = ColorSpace::TransferID::kBT1361_ECG;
+//      break;
+//    case AVCOL_TRC_IEC61966_2_1:
+//      transfer = ColorSpace::TransferID::kIEC61966_2_1;
+//      break;
+//    case AVCOL_TRC_BT2020_10:
+//      transfer = ColorSpace::TransferID::kBT2020_10;
+//      break;
+//    case AVCOL_TRC_BT2020_12:
+//      transfer = ColorSpace::TransferID::kBT2020_12;
+//      break;
+//    case AVCOL_TRC_SMPTE2084:
+//      transfer = ColorSpace::TransferID::kSMPTEST2084;
+//      break;
+//    case AVCOL_TRC_SMPTE428:
+//      transfer = ColorSpace::TransferID::kSMPTEST428;
+//      break;
+//    case AVCOL_TRC_ARIB_STD_B67:
+//      transfer = ColorSpace::TransferID::kARIB_STD_B67;
+//      break;
+//    case AVCOL_TRC_RESERVED0:
+//    case AVCOL_TRC_UNSPECIFIED:
+//    case AVCOL_TRC_RESERVED:
+//    default:
+//      break;
+//  }
+//
+//  ColorSpace::MatrixID matrix = ColorSpace::MatrixID::kUnspecified;
+//  switch (codec->colorspace) {
+//    case AVCOL_SPC_RGB:
+//      matrix = ColorSpace::MatrixID::kRGB;
+//      break;
+//    case AVCOL_SPC_BT709:
+//      matrix = ColorSpace::MatrixID::kBT709;
+//      break;
+//    case AVCOL_SPC_FCC:
+//      matrix = ColorSpace::MatrixID::kFCC;
+//      break;
+//    case AVCOL_SPC_BT470BG:
+//      matrix = ColorSpace::MatrixID::kBT470BG;
+//      break;
+//    case AVCOL_SPC_SMPTE170M:
+//      matrix = ColorSpace::MatrixID::kSMPTE170M;
+//      break;
+//    case AVCOL_SPC_SMPTE240M:
+//      matrix = ColorSpace::MatrixID::kSMPTE240M;
+//      break;
+//    case AVCOL_SPC_YCGCO:
+//      matrix = ColorSpace::MatrixID::kYCOCG;
+//      break;
+//    case AVCOL_SPC_BT2020_NCL:
+//      matrix = ColorSpace::MatrixID::kBT2020_NCL;
+//      break;
+//    case AVCOL_SPC_BT2020_CL:
+//      matrix = ColorSpace::MatrixID::kBT2020_CL;
+//      break;
+//    case AVCOL_SPC_SMPTE2085:
+//      matrix = ColorSpace::MatrixID::kSMPTE2085;
+//      break;
+//    case AVCOL_SPC_CHROMA_DERIVED_NCL:
+//    case AVCOL_SPC_CHROMA_DERIVED_CL:
+//    case AVCOL_SPC_ICTCP:
+//    case AVCOL_SPC_UNSPECIFIED:
+//    case AVCOL_SPC_RESERVED:
+//    default:
+//      break;
+//  }
+//
+//  ColorSpace::RangeID range = ColorSpace::RangeID::kInvalid;
+//  switch (codec->color_range) {
+//    case AVCOL_RANGE_MPEG:
+//      range = ColorSpace::RangeID::kLimited;
+//      break;
+//    case AVCOL_RANGE_JPEG:
+//      range = ColorSpace::RangeID::kFull;
+//      break;
+//    case AVCOL_RANGE_UNSPECIFIED:
+//    default:
+//      break;
+//  }
+//  return ColorSpace(primaries, transfer, matrix, range);
+//}
 
 }  // namespace webrtc
 
diff --git a/modules/video_coding/codecs/h264/h264_color_space.h b/modules/video_coding/codecs/h264/h264_color_space.h
index 392ccaf563..8f04c66473 100644
--- a/modules/video_coding/codecs/h264/h264_color_space.h
+++ b/modules/video_coding/codecs/h264/h264_color_space.h
@@ -16,20 +16,20 @@
 // #ifdef unless needed and tested.
 #ifdef WEBRTC_USE_H264
 
-#if defined(WEBRTC_WIN) && !defined(__clang__)
-#error "See: bugs.webrtc.org/9213#c13."
-#endif
+//#if defined(WEBRTC_WIN) && !defined(__clang__)
+//#error "See: bugs.webrtc.org/9213#c13."
+//#endif
 
 #include "api/video/color_space.h"
 
-extern "C" {
-#include "third_party/ffmpeg/libavcodec/avcodec.h"
-}  // extern "C"
+//extern "C" {
+//#include "third_party/ffmpeg/libavcodec/avcodec.h"
+//}  // extern "C"
 
 namespace webrtc {
 
 // Helper class for extracting color space information from H264 stream.
-ColorSpace ExtractH264ColorSpace(AVCodecContext* codec);
+//ColorSpace ExtractH264ColorSpace(AVCodecContext* codec);
 
 }  // namespace webrtc
 
diff --git a/modules/video_coding/codecs/h264/h264_decoder_impl.cc b/modules/video_coding/codecs/h264/h264_decoder_impl.cc
index fa5af98ef2..8ce82b3075 100644
--- a/modules/video_coding/codecs/h264/h264_decoder_impl.cc
+++ b/modules/video_coding/codecs/h264/h264_decoder_impl.cc
@@ -20,11 +20,48 @@
 #include <limits>
 #include <memory>
 
-extern "C" {
-#include "third_party/ffmpeg/libavcodec/avcodec.h"
-#include "third_party/ffmpeg/libavformat/avformat.h"
-#include "third_party/ffmpeg/libavutil/imgutils.h"
-}  // extern "C"
+//
+// WebRTC-UWP Begin
+//
+#include <Windows.h>
+#include <codecapi.h>
+#include <mfapi.h>
+#include <mfidl.h>
+#include <mfreadwrite.h>
+#include <ppltasks.h>
+#include <robuffer.h>
+#include <wrl.h>
+#include <wrl\implements.h>
+#include <iomanip>
+//#include "../Utils/Utils.h"
+#include "common_video/include/video_frame_buffer.h"
+#include "libyuv/convert.h"
+#include "modules/video_coding/include/video_codec_interface.h"
+#include "rtc_base/checks.h"
+#include "rtc_base/logging.h"
+
+#pragma comment(lib, "mfreadwrite")
+#pragma comment(lib, "mfplat")
+#pragma comment(lib, "mfuuid.lib")
+
+using Microsoft::WRL::ComPtr;
+#define ON_SUCCEEDED(act)                      \
+  if (SUCCEEDED(hr)) {                         \
+    hr = act;                                  \
+    if (FAILED(hr)) {                          \
+      RTC_LOG(LS_WARNING) << "ERROR:" << #act; \
+    }                                          \
+  }
+//
+// WebRTC-UWP End
+//
+
+
+//extern "C" {
+//#include "third_party/ffmpeg/libavcodec/avcodec.h"
+//#include "third_party/ffmpeg/libavformat/avformat.h"
+//#include "third_party/ffmpeg/libavutil/imgutils.h"
+//}  // extern "C"
 
 #include "api/video/color_space.h"
 #include "api/video/i010_buffer.h"
@@ -42,335 +79,911 @@ namespace webrtc {
 
 namespace {
 
-const AVPixelFormat kPixelFormatDefault = AV_PIX_FMT_YUV420P;
-const AVPixelFormat kPixelFormatFullRange = AV_PIX_FMT_YUVJ420P;
-const size_t kYPlaneIndex = 0;
-const size_t kUPlaneIndex = 1;
-const size_t kVPlaneIndex = 2;
+//const AVPixelFormat kPixelFormatDefault = AV_PIX_FMT_YUV420P;
+//const AVPixelFormat kPixelFormatFullRange = AV_PIX_FMT_YUVJ420P;
+//const size_t kYPlaneIndex = 0;
+//const size_t kUPlaneIndex = 1;
+//const size_t kVPlaneIndex = 2;
 
 // Used by histograms. Values of entries should not be changed.
-enum H264DecoderImplEvent {
-  kH264DecoderEventInit = 0,
-  kH264DecoderEventError = 1,
-  kH264DecoderEventMax = 16,
-};
+//enum H264DecoderImplEvent {
+//  kH264DecoderEventInit = 0,
+//  kH264DecoderEventError = 1,
+//  kH264DecoderEventMax = 16,
+//};
 
 }  // namespace
 
-int H264DecoderImpl::AVGetBuffer2(AVCodecContext* context,
-                                  AVFrame* av_frame,
-                                  int flags) {
-  // Set in |InitDecode|.
-  H264DecoderImpl* decoder = static_cast<H264DecoderImpl*>(context->opaque);
-  // DCHECK values set in |InitDecode|.
-  RTC_DCHECK(decoder);
-  // Necessary capability to be allowed to provide our own buffers.
-  RTC_DCHECK(context->codec->capabilities | AV_CODEC_CAP_DR1);
-
-  // Limited or full range YUV420 is expected.
-  RTC_CHECK(context->pix_fmt == kPixelFormatDefault ||
-            context->pix_fmt == kPixelFormatFullRange);
-
-  // |av_frame->width| and |av_frame->height| are set by FFmpeg. These are the
-  // actual image's dimensions and may be different from |context->width| and
-  // |context->coded_width| due to reordering.
-  int width = av_frame->width;
-  int height = av_frame->height;
-  // See |lowres|, if used the decoder scales the image by 1/2^(lowres). This
-  // has implications on which resolutions are valid, but we don't use it.
-  RTC_CHECK_EQ(context->lowres, 0);
-  // Adjust the |width| and |height| to values acceptable by the decoder.
-  // Without this, FFmpeg may overflow the buffer. If modified, |width| and/or
-  // |height| are larger than the actual image and the image has to be cropped
-  // (top-left corner) after decoding to avoid visible borders to the right and
-  // bottom of the actual image.
-  avcodec_align_dimensions(context, &width, &height);
-
-  RTC_CHECK_GE(width, 0);
-  RTC_CHECK_GE(height, 0);
-  int ret = av_image_check_size(static_cast<unsigned int>(width),
-                                static_cast<unsigned int>(height), 0, nullptr);
-  if (ret < 0) {
-    RTC_LOG(LS_ERROR) << "Invalid picture size " << width << "x" << height;
-    decoder->ReportError();
-    return ret;
-  }
+//int H264DecoderImpl::AVGetBuffer2(AVCodecContext* context,
+//                                  AVFrame* av_frame,
+//                                  int flags) {
+//  // Set in |InitDecode|.
+//  H264DecoderImpl* decoder = static_cast<H264DecoderImpl*>(context->opaque);
+//  // DCHECK values set in |InitDecode|.
+//  RTC_DCHECK(decoder);
+//  // Necessary capability to be allowed to provide our own buffers.
+//  RTC_DCHECK(context->codec->capabilities | AV_CODEC_CAP_DR1);
+//
+//  // Limited or full range YUV420 is expected.
+//  RTC_CHECK(context->pix_fmt == kPixelFormatDefault ||
+//            context->pix_fmt == kPixelFormatFullRange);
+//
+//  // |av_frame->width| and |av_frame->height| are set by FFmpeg. These are the
+//  // actual image's dimensions and may be different from |context->width| and
+//  // |context->coded_width| due to reordering.
+//  int width = av_frame->width;
+//  int height = av_frame->height;
+//  // See |lowres|, if used the decoder scales the image by 1/2^(lowres). This
+//  // has implications on which resolutions are valid, but we don't use it.
+//  RTC_CHECK_EQ(context->lowres, 0);
+//  // Adjust the |width| and |height| to values acceptable by the decoder.
+//  // Without this, FFmpeg may overflow the buffer. If modified, |width| and/or
+//  // |height| are larger than the actual image and the image has to be cropped
+//  // (top-left corner) after decoding to avoid visible borders to the right and
+//  // bottom of the actual image.
+//  avcodec_align_dimensions(context, &width, &height);
+//
+//  RTC_CHECK_GE(width, 0);
+//  RTC_CHECK_GE(height, 0);
+//  int ret = av_image_check_size(static_cast<unsigned int>(width),
+//                                static_cast<unsigned int>(height), 0, nullptr);
+//  if (ret < 0) {
+//    RTC_LOG(LS_ERROR) << "Invalid picture size " << width << "x" << height;
+//    decoder->ReportError();
+//    return ret;
+//  }
+//
+//  // The video frame is stored in |frame_buffer|. |av_frame| is FFmpeg's version
+//  // of a video frame and will be set up to reference |frame_buffer|'s data.
+//
+//  // FFmpeg expects the initial allocation to be zero-initialized according to
+//  // http://crbug.com/390941. Our pool is set up to zero-initialize new buffers.
+//  // TODO(nisse): Delete that feature from the video pool, instead add
+//  // an explicit call to InitializeData here.
+//  rtc::scoped_refptr<I420Buffer> frame_buffer =
+//      decoder->pool_.CreateBuffer(width, height);
+//
+//  int y_size = width * height;
+//  int uv_size = frame_buffer->ChromaWidth() * frame_buffer->ChromaHeight();
+//  // DCHECK that we have a continuous buffer as is required.
+//  RTC_DCHECK_EQ(frame_buffer->DataU(), frame_buffer->DataY() + y_size);
+//  RTC_DCHECK_EQ(frame_buffer->DataV(), frame_buffer->DataU() + uv_size);
+//  int total_size = y_size + 2 * uv_size;
+//
+//  av_frame->format = context->pix_fmt;
+//  av_frame->reordered_opaque = context->reordered_opaque;
+//
+//  // Set |av_frame| members as required by FFmpeg.
+//  av_frame->data[kYPlaneIndex] = frame_buffer->MutableDataY();
+//  av_frame->linesize[kYPlaneIndex] = frame_buffer->StrideY();
+//  av_frame->data[kUPlaneIndex] = frame_buffer->MutableDataU();
+//  av_frame->linesize[kUPlaneIndex] = frame_buffer->StrideU();
+//  av_frame->data[kVPlaneIndex] = frame_buffer->MutableDataV();
+//  av_frame->linesize[kVPlaneIndex] = frame_buffer->StrideV();
+//  RTC_DCHECK_EQ(av_frame->extended_data, av_frame->data);
+//
+//  // Create a VideoFrame object, to keep a reference to the buffer.
+//  // TODO(nisse): The VideoFrame's timestamp and rotation info is not used.
+//  // Refactor to do not use a VideoFrame object at all.
+//  av_frame->buf[0] = av_buffer_create(
+//      av_frame->data[kYPlaneIndex], total_size, AVFreeBuffer2,
+//      static_cast<void*>(
+//          std::make_unique<VideoFrame>(VideoFrame::Builder()
+//                                           .set_video_frame_buffer(frame_buffer)
+//                                           .set_rotation(kVideoRotation_0)
+//                                           .set_timestamp_us(0)
+//                                           .build())
+//              .release()),
+//      0);
+//  RTC_CHECK(av_frame->buf[0]);
+//  return 0;
+//}
+
+//void H264DecoderImpl::AVFreeBuffer2(void* opaque, uint8_t* data) {
+//  // The buffer pool recycles the buffer used by |video_frame| when there are no
+//  // more references to it. |video_frame| is a thin buffer holder and is not
+//  // recycled.
+//  VideoFrame* video_frame = static_cast<VideoFrame*>(opaque);
+//  delete video_frame;
+//}
+
+H264DecoderImpl::H264DecoderImpl()
+//    : pool_(true),
+//      decoded_image_callback_(nullptr),
+//      has_reported_init_(false),
+//      has_reported_error_(false)
+
+//
+// WebRTC-UWP Begin
+//
+    : buffer_pool_(false, 300), /* max_number_of_buffers*/
+      width_(absl::nullopt),
+      height_(absl::nullopt),
+      decode_complete_callback_(nullptr)
+//
+// WebRTC-UWP End
+//
+{}
+
+H264DecoderImpl::~H264DecoderImpl() {
+//  Release();
 
-  // The video frame is stored in |frame_buffer|. |av_frame| is FFmpeg's version
-  // of a video frame and will be set up to reference |frame_buffer|'s data.
-
-  // FFmpeg expects the initial allocation to be zero-initialized according to
-  // http://crbug.com/390941. Our pool is set up to zero-initialize new buffers.
-  // TODO(nisse): Delete that feature from the video pool, instead add
-  // an explicit call to InitializeData here.
-  rtc::scoped_refptr<I420Buffer> frame_buffer =
-      decoder->pool_.CreateBuffer(width, height);
-
-  int y_size = width * height;
-  int uv_size = frame_buffer->ChromaWidth() * frame_buffer->ChromaHeight();
-  // DCHECK that we have a continuous buffer as is required.
-  RTC_DCHECK_EQ(frame_buffer->DataU(), frame_buffer->DataY() + y_size);
-  RTC_DCHECK_EQ(frame_buffer->DataV(), frame_buffer->DataU() + uv_size);
-  int total_size = y_size + 2 * uv_size;
-
-  av_frame->format = context->pix_fmt;
-  av_frame->reordered_opaque = context->reordered_opaque;
-
-  // Set |av_frame| members as required by FFmpeg.
-  av_frame->data[kYPlaneIndex] = frame_buffer->MutableDataY();
-  av_frame->linesize[kYPlaneIndex] = frame_buffer->StrideY();
-  av_frame->data[kUPlaneIndex] = frame_buffer->MutableDataU();
-  av_frame->linesize[kUPlaneIndex] = frame_buffer->StrideU();
-  av_frame->data[kVPlaneIndex] = frame_buffer->MutableDataV();
-  av_frame->linesize[kVPlaneIndex] = frame_buffer->StrideV();
-  RTC_DCHECK_EQ(av_frame->extended_data, av_frame->data);
-
-  // Create a VideoFrame object, to keep a reference to the buffer.
-  // TODO(nisse): The VideoFrame's timestamp and rotation info is not used.
-  // Refactor to do not use a VideoFrame object at all.
-  av_frame->buf[0] = av_buffer_create(
-      av_frame->data[kYPlaneIndex], total_size, AVFreeBuffer2,
-      static_cast<void*>(
-          std::make_unique<VideoFrame>(VideoFrame::Builder()
-                                           .set_video_frame_buffer(frame_buffer)
-                                           .set_rotation(kVideoRotation_0)
-                                           .set_timestamp_us(0)
-                                           .build())
-              .release()),
-      0);
-  RTC_CHECK(av_frame->buf[0]);
-  return 0;
+  //
+  // WebRTC-UWP Begin
+  //
+  OutputDebugString(L"H264DecoderImpl::~WinUWPH264DecoderImpl()\n");
+  Release();
+  //
+  // WebRTC-UWP End
+  //
 }
 
-void H264DecoderImpl::AVFreeBuffer2(void* opaque, uint8_t* data) {
-  // The buffer pool recycles the buffer used by |video_frame| when there are no
-  // more references to it. |video_frame| is a thin buffer holder and is not
-  // recycled.
-  VideoFrame* video_frame = static_cast<VideoFrame*>(opaque);
-  delete video_frame;
+//
+// WebRTC-UWP Begin
+//
+HRESULT ConfigureOutputMediaType(ComPtr<IMFTransform> decoder,
+                                 GUID media_type,
+                                 bool* type_found) {
+  HRESULT hr = S_OK;
+  *type_found = false;
+
+  int type = 0;
+  while (true) {
+    ComPtr<IMFMediaType> output_media;
+    ON_SUCCEEDED(decoder->GetOutputAvailableType(0, type, &output_media));
+    if (hr == MF_E_NO_MORE_TYPES)
+      return S_OK;
+
+    GUID cur_type;
+    ON_SUCCEEDED(output_media->GetGUID(MF_MT_SUBTYPE, &cur_type));
+    if (FAILED(hr))
+      return hr;
+
+    if (cur_type == media_type) {
+      hr = decoder->SetOutputType(0, output_media.Get(), 0);
+      ON_SUCCEEDED(*type_found = true);
+      return hr;
+    }
+
+    type++;
+  }
 }
 
-H264DecoderImpl::H264DecoderImpl()
-    : pool_(true),
-      decoded_image_callback_(nullptr),
-      has_reported_init_(false),
-      has_reported_error_(false) {}
+HRESULT CreateInputMediaType(IMFMediaType** pp_input_media,
+                             absl::optional<UINT32> img_width,
+                             absl::optional<UINT32> img_height,
+                             absl::optional<UINT32> frame_rate) {
+  HRESULT hr = MFCreateMediaType(pp_input_media);
+
+  IMFMediaType* input_media = *pp_input_media;
+  ON_SUCCEEDED(input_media->SetGUID(MF_MT_MAJOR_TYPE, MFMediaType_Video));
+  ON_SUCCEEDED(input_media->SetGUID(MF_MT_SUBTYPE, MFVideoFormat_H264));
+  ON_SUCCEEDED(
+      MFSetAttributeRatio(input_media, MF_MT_PIXEL_ASPECT_RATIO, 1, 1));
+  ON_SUCCEEDED(input_media->SetUINT32(
+      MF_MT_INTERLACE_MODE, MFVideoInterlace_MixedInterlaceOrProgressive));
+
+  if (frame_rate.has_value()) {
+    ON_SUCCEEDED(MFSetAttributeRatio(input_media, MF_MT_FRAME_RATE,
+                                     frame_rate.value(), 1));
+  }
 
-H264DecoderImpl::~H264DecoderImpl() {
-  Release();
+  if (img_width.has_value() && img_height.has_value()) {
+    ON_SUCCEEDED(MFSetAttributeSize(input_media, MF_MT_FRAME_SIZE,
+                                    img_width.value(), img_height.value()));
+  }
+
+  return hr;
 }
+//
+// WebRTC-UWP End
+//
+
+
+
 
 int32_t H264DecoderImpl::InitDecode(const VideoCodec* codec_settings,
                                     int32_t number_of_cores) {
-  ReportInit();
+  //ReportInit();
   if (codec_settings && codec_settings->codecType != kVideoCodecH264) {
-    ReportError();
+    //ReportError();
     return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
   }
 
   // Release necessary in case of re-initializing.
   int32_t ret = Release();
   if (ret != WEBRTC_VIDEO_CODEC_OK) {
-    ReportError();
+    //ReportError();
     return ret;
   }
-  RTC_DCHECK(!av_context_);
 
-  // Initialize AVCodecContext.
-  av_context_.reset(avcodec_alloc_context3(nullptr));
+  //
+  // WebRTC-UWP Begin
+  //
 
-  av_context_->codec_type = AVMEDIA_TYPE_VIDEO;
-  av_context_->codec_id = AV_CODEC_ID_H264;
-  if (codec_settings) {
-    av_context_->coded_width = codec_settings->width;
-    av_context_->coded_height = codec_settings->height;
+  RTC_LOG(LS_INFO) << "H264DecoderImpl::InitDecode()\n";
+
+  width_ = codec_settings->width > 0
+               ? absl::optional<UINT32>(codec_settings->width)
+               : absl::nullopt;
+  height_ = codec_settings->height > 0
+                ? absl::optional<UINT32>(codec_settings->height)
+                : absl::nullopt;
+
+  HRESULT hr = S_OK;
+  ON_SUCCEEDED(CoInitializeEx(0, COINIT_APARTMENTTHREADED));
+  ON_SUCCEEDED(MFStartup(MF_VERSION, 0));
+
+  ON_SUCCEEDED(CoCreateInstance(CLSID_MSH264DecoderMFT, nullptr,
+                                CLSCTX_INPROC_SERVER, IID_IUnknown,
+                                (void**)&decoder_));
+
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR) << "Init failure: could not create Media Foundation H264 "
+                         "decoder instance.";
+    return WEBRTC_VIDEO_CODEC_ERROR;
+  }
+
+  // Try set decoder attributes
+  ComPtr<IMFAttributes> decoder_attrs;
+  ON_SUCCEEDED(decoder_->GetAttributes(decoder_attrs.GetAddressOf()));
+
+  if (SUCCEEDED(hr)) {
+    ON_SUCCEEDED(decoder_attrs->SetUINT32(CODECAPI_AVLowLatencyMode, TRUE));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_WARNING)
+          << "Init warning: failed to set low latency in H264 decoder.";
+      hr = S_OK;
+    }
+
+    ON_SUCCEEDED(
+        decoder_attrs->SetUINT32(CODECAPI_AVDecVideoAcceleration_H264, TRUE));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_WARNING)
+          << "Init warning: failed to set HW accel in H264 decoder.";
+    }
+  }
+
+  // Clear any error from try set attributes
+  hr = S_OK;
+
+  ComPtr<IMFMediaType> input_media;
+  ON_SUCCEEDED(CreateInputMediaType(
+      input_media.GetAddressOf(), width_, height_,
+      codec_settings->maxFramerate > 0
+          ? absl::optional<UINT32>(codec_settings->maxFramerate)
+          : absl::nullopt));
+
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR) << "Init failure: could not create input media type.";
+    return WEBRTC_VIDEO_CODEC_ERROR;
   }
-  av_context_->pix_fmt = kPixelFormatDefault;
-  av_context_->extradata = nullptr;
-  av_context_->extradata_size = 0;
-
-  // If this is ever increased, look at |av_context_->thread_safe_callbacks| and
-  // make it possible to disable the thread checker in the frame buffer pool.
-  av_context_->thread_count = 1;
-  av_context_->thread_type = FF_THREAD_SLICE;
-
-  // Function used by FFmpeg to get buffers to store decoded frames in.
-  av_context_->get_buffer2 = AVGetBuffer2;
-  // |get_buffer2| is called with the context, there |opaque| can be used to get
-  // a pointer |this|.
-  av_context_->opaque = this;
-
-  AVCodec* codec = avcodec_find_decoder(av_context_->codec_id);
-  if (!codec) {
-    // This is an indication that FFmpeg has not been initialized or it has not
-    // been compiled/initialized with the correct set of codecs.
-    RTC_LOG(LS_ERROR) << "FFmpeg H.264 decoder not found.";
-    Release();
-    ReportError();
+
+  // Register the input type with the decoder
+  ON_SUCCEEDED(decoder_->SetInputType(0, input_media.Get(), 0));
+
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR)
+        << "Init failure: failed to set input media type on decoder.";
     return WEBRTC_VIDEO_CODEC_ERROR;
   }
-  int res = avcodec_open2(av_context_.get(), codec, nullptr);
-  if (res < 0) {
-    RTC_LOG(LS_ERROR) << "avcodec_open2 error: " << res;
-    Release();
-    ReportError();
+
+  // Assert MF supports NV12 output
+  bool suitable_type_found;
+  ON_SUCCEEDED(ConfigureOutputMediaType(decoder_, MFVideoFormat_NV12,
+                                        &suitable_type_found));
+
+  if (FAILED(hr) || !suitable_type_found) {
+    RTC_LOG(LS_ERROR) << "Init failure: failed to find a valid output media "
+                         "type for decoding.";
     return WEBRTC_VIDEO_CODEC_ERROR;
   }
 
-  av_frame_.reset(av_frame_alloc());
-  return WEBRTC_VIDEO_CODEC_OK;
+  DWORD status;
+  ON_SUCCEEDED(decoder_->GetInputStatus(0, &status));
+
+  // Validate that decoder is up and running
+  if (SUCCEEDED(hr)) {
+    if (MFT_INPUT_STATUS_ACCEPT_DATA != status)
+      // H.264 decoder MFT is not accepting data
+      return WEBRTC_VIDEO_CODEC_ERROR;
+  }
+
+  ON_SUCCEEDED(decoder_->ProcessMessage(MFT_MESSAGE_COMMAND_FLUSH, NULL));
+  ON_SUCCEEDED(
+      decoder_->ProcessMessage(MFT_MESSAGE_NOTIFY_BEGIN_STREAMING, NULL));
+  ON_SUCCEEDED(
+      decoder_->ProcessMessage(MFT_MESSAGE_NOTIFY_START_OF_STREAM, NULL));
+
+  inited_ = true;
+  return SUCCEEDED(hr) ? WEBRTC_VIDEO_CODEC_OK : WEBRTC_VIDEO_CODEC_ERROR;
+
+  //
+  // WebRTC-UWP End
+  //
+
+
+//  RTC_DCHECK(!av_context_);
+
+//  // Initialize AVCodecContext.
+//  av_context_.reset(avcodec_alloc_context3(nullptr));
+//
+//  av_context_->codec_type = AVMEDIA_TYPE_VIDEO;
+//  av_context_->codec_id = AV_CODEC_ID_H264;
+//  if (codec_settings) {
+//    av_context_->coded_width = codec_settings->width;
+//    av_context_->coded_height = codec_settings->height;
+//  }
+//  av_context_->pix_fmt = kPixelFormatDefault;
+//  av_context_->extradata = nullptr;
+//  av_context_->extradata_size = 0;
+//
+//  // If this is ever increased, look at |av_context_->thread_safe_callbacks| and
+//  // make it possible to disable the thread checker in the frame buffer pool.
+//  av_context_->thread_count = 1;
+//  av_context_->thread_type = FF_THREAD_SLICE;
+//
+//  // Function used by FFmpeg to get buffers to store decoded frames in.
+//  av_context_->get_buffer2 = AVGetBuffer2;
+//  // |get_buffer2| is called with the context, there |opaque| can be used to get
+//  // a pointer |this|.
+//  av_context_->opaque = this;
+//
+//  AVCodec* codec = avcodec_find_decoder(av_context_->codec_id);
+//  if (!codec) {
+//    // This is an indication that FFmpeg has not been initialized or it has not
+//    // been compiled/initialized with the correct set of codecs.
+//    RTC_LOG(LS_ERROR) << "FFmpeg H.264 decoder not found.";
+//    Release();
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERROR;
+//  }
+//  int res = avcodec_open2(av_context_.get(), codec, nullptr);
+//  if (res < 0) {
+//    RTC_LOG(LS_ERROR) << "avcodec_open2 error: " << res;
+//    Release();
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERROR;
+//  }
+//
+//  av_frame_.reset(av_frame_alloc());
+//  return WEBRTC_VIDEO_CODEC_OK;
+}
+
+//
+// WebRTC-UWP Begin
+//
+/**
+ * Workaround [MF H264 bug: Output status is never set, even when ready]
+ *  => For now, always mark "ready" (results in extra buffer alloc/dealloc).
+ */
+HRESULT GetOutputStatus(ComPtr<IMFTransform> decoder, DWORD* output_status) {
+  HRESULT hr = decoder->GetOutputStatus(output_status);
+
+  // Don't MFT trust output status for now.
+  *output_status = MFT_OUTPUT_STATUS_SAMPLE_READY;
+  return hr;
 }
 
+/**
+ * Note: expected to return MF_E_TRANSFORM_NEED_MORE_INPUT and
+ *       MF_E_TRANSFORM_STREAM_CHANGE which must be handled by caller.
+ */
+HRESULT H264DecoderImpl::FlushFrames(uint32_t rtp_timestamp,
+                                           uint64_t ntp_time_ms) {
+  HRESULT hr;
+  DWORD output_status;
+
+  while (SUCCEEDED(hr = GetOutputStatus(decoder_, &output_status)) &&
+         output_status == MFT_OUTPUT_STATUS_SAMPLE_READY) {
+    // Get needed size of our output buffer
+    MFT_OUTPUT_STREAM_INFO strm_info;
+    ON_SUCCEEDED(decoder_->GetOutputStreamInfo(0, &strm_info));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_ERROR) << "Decode failure: failed to get output stream info.";
+      return hr;
+    }
+
+    // Create output sample
+    ComPtr<IMFMediaBuffer> out_buffer;
+    ON_SUCCEEDED(MFCreateMemoryBuffer(strm_info.cbSize, &out_buffer));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_ERROR)
+          << "Decode failure: output image memory buffer creation failed.";
+      return hr;
+    }
+
+    ComPtr<IMFSample> out_sample;
+    ON_SUCCEEDED(MFCreateSample(&out_sample));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_ERROR) << "Decode failure: output in_sample creation failed.";
+      return hr;
+    }
+
+    ON_SUCCEEDED(out_sample->AddBuffer(out_buffer.Get()));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_ERROR)
+          << "Decode failure: failed to add buffer to output in_sample.";
+      return hr;
+    }
+
+    // Create output buffer description
+    MFT_OUTPUT_DATA_BUFFER output_data_buffer;
+    output_data_buffer.dwStatus = 0;
+    output_data_buffer.dwStreamID = 0;
+    output_data_buffer.pEvents = nullptr;
+    output_data_buffer.pSample = out_sample.Get();
+
+    // Invoke the Media Foundation decoder
+    // Note: we don't use ON_SUCCEEDED here since ProcessOutput returns
+    //       MF_E_TRANSFORM_NEED_MORE_INPUT often (too many log messages).
+    DWORD status;
+    hr = decoder_->ProcessOutput(0, 1, &output_data_buffer, &status);
+
+    if (FAILED(hr))
+      return hr; /* can return MF_E_TRANSFORM_NEED_MORE_INPUT or
+                    MF_E_TRANSFORM_STREAM_CHANGE (entirely acceptable) */
+
+    // Copy raw output sample data to video frame buffer.
+    ComPtr<IMFMediaBuffer> src_buffer;
+    ON_SUCCEEDED(out_sample->ConvertToContiguousBuffer(&src_buffer));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_ERROR) << "Decode failure: failed to get contiguous buffer.";
+      return hr;
+    }
+
+    uint32_t width, height;
+    if (width_.has_value() && height_.has_value()) {
+      width = width_.value();
+      height = height_.value();
+    } else {
+      // Query the size from MF output media type
+      ComPtr<IMFMediaType> output_type;
+      ON_SUCCEEDED(
+          decoder_->GetOutputCurrentType(0, output_type.GetAddressOf()));
+
+      ON_SUCCEEDED(MFGetAttributeSize(output_type.Get(), MF_MT_FRAME_SIZE,
+                                      &width, &height));
+      if (FAILED(hr)) {
+        RTC_LOG(LS_ERROR) << "Decode failure: could not read image dimensions "
+                             "from Media Foundation, so the video frame buffer "
+                             "size can not be determined.";
+        return hr;
+      }
+
+      // Update members to avoid querying unnecessarily
+      width_.emplace(width);
+      height_.emplace(height);
+    }
+
+    rtc::scoped_refptr<I420Buffer> buffer =
+        buffer_pool_.CreateBuffer(width, height);
+
+    if (!buffer.get()) {
+      // Pool has too many pending frames.
+      RTC_LOG(LS_WARNING) << "Decode warning: too many frames. Dropping frame.";
+      return WEBRTC_VIDEO_CODEC_NO_OUTPUT;
+    }
+
+    DWORD cur_length;
+    ON_SUCCEEDED(src_buffer->GetCurrentLength(&cur_length));
+    if (FAILED(hr)) {
+      RTC_LOG(LS_ERROR) << "Decode failure: could not get buffer length.";
+      return hr;
+    }
+
+    if (cur_length > 0) {
+      BYTE* src_data;
+      DWORD max_len, cur_len;
+      ON_SUCCEEDED(src_buffer->Lock(&src_data, &max_len, &cur_len));
+      if (FAILED(hr)) {
+        RTC_LOG(LS_ERROR) << "Decode failure: could lock buffer for copying.";
+        return hr;
+      }
+
+      // Convert NV12 to I420. Y and UV sections have same stride in NV12
+      // (width). The size of the Y section is the size of the frame, since Y
+      // luminance values are 8-bits each.
+      libyuv::NV12ToI420(src_data, width, src_data + (width * height), width,
+                         buffer->MutableDataY(), buffer->StrideY(),
+                         buffer->MutableDataU(), buffer->StrideU(),
+                         buffer->MutableDataV(), buffer->StrideV(), width,
+                         height);
+
+      ON_SUCCEEDED(src_buffer->Unlock());
+      if (FAILED(hr))
+        return hr;
+    }
+
+    // LONGLONG sample_time; /* unused */
+    // ON_SUCCEEDED(spOutSample->GetSampleTime(&sample_time));
+
+    // TODO: Ideally, we should convert sample_time (above) back to 90khz + base
+    // and use it in place of rtp_timestamp, since MF may interpolate it.
+    // Instead, we ignore the MFT sample time out, using rtp from in frame that
+    // triggered this decoded frame.
+    VideoFrame decoded_frame(buffer, rtp_timestamp, 0, kVideoRotation_0);
+
+    // Use ntp time from the earliest frame
+    decoded_frame.set_ntp_time_ms(ntp_time_ms);
+
+    // Emit image to downstream
+    if (decode_complete_callback_ != nullptr) {
+      decode_complete_callback_->Decoded(decoded_frame, absl::nullopt,
+                                         absl::nullopt);
+    }
+  }
+
+  return hr;
+}
+
+/**
+ * Note: acceptable to return MF_E_NOTACCEPTING (though it shouldn't since
+ * last loop should've flushed)
+ */
+HRESULT H264DecoderImpl::EnqueueFrame(const EncodedImage& input_image,
+                                            bool missing_frames) {
+  HRESULT hr = S_OK;
+
+  // Create a MF buffer from our data
+  ComPtr<IMFMediaBuffer> in_buffer;
+  //ON_SUCCEEDED(MFCreateMemoryBuffer(input_image._length, &in_buffer));
+  ON_SUCCEEDED(MFCreateMemoryBuffer(input_image.size(), &in_buffer));
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR)
+        << "Decode failure: input image memory buffer creation failed.";
+    return hr;
+  }
+
+  DWORD max_len, cur_len;
+  BYTE* data;
+  ON_SUCCEEDED(in_buffer->Lock(&data, &max_len, &cur_len));
+  if (FAILED(hr))
+    return hr;
+
+  //memcpy(data, input_image._buffer, input_image._length);
+  memcpy(data, input_image.buffer(), input_image.size());
+
+  ON_SUCCEEDED(in_buffer->Unlock());
+  if (FAILED(hr))
+    return hr;
+
+  //ON_SUCCEEDED(in_buffer->SetCurrentLength(input_image._length));
+  ON_SUCCEEDED(in_buffer->SetCurrentLength(input_image.size()));
+  if (FAILED(hr))
+    return hr;
+
+  // Create a sample from media buffer
+  ComPtr<IMFSample> in_sample;
+  ON_SUCCEEDED(MFCreateSample(&in_sample));
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR) << "Decode failure: input in_sample creation failed.";
+    return hr;
+  }
+
+  ON_SUCCEEDED(in_sample->AddBuffer(in_buffer.Get()));
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR)
+        << "Decode failure: failed to add buffer to input in_sample.";
+    return hr;
+  }
+
+  int64_t sample_time_ms;
+  if (first_frame_rtp_ == 0) {
+    first_frame_rtp_ = input_image.Timestamp();
+    sample_time_ms = 0;
+  } else {
+    // Convert from 90 khz, rounding to nearest ms.
+    sample_time_ms =
+        (static_cast<uint64_t>(input_image.Timestamp()) - first_frame_rtp_) /
+            90.0 +
+        0.5f;
+  }
+
+  ON_SUCCEEDED(in_sample->SetSampleTime(
+      sample_time_ms *
+      10000 /* convert milliseconds to 100-nanosecond unit */));
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR)
+        << "Decode failure: failed to set in_sample time on input in_sample.";
+    return hr;
+  }
+
+  // Set sample attributes
+  ComPtr<IMFAttributes> sample_attrs;
+  ON_SUCCEEDED(in_sample.As(&sample_attrs));
+
+  if (FAILED(hr)) {
+    RTC_LOG(LS_ERROR)
+        << "Decode warning: failed to set image attributes for frame.";
+    hr = S_OK;
+  } else {
+    //if (input_image._frameType == kVideoFrameKey &&
+    if (input_image._frameType == VideoFrameType::kVideoFrameKey &&
+        input_image._completeFrame) {
+      ON_SUCCEEDED(sample_attrs->SetUINT32(MFSampleExtension_CleanPoint, TRUE));
+      hr = S_OK;
+    }
+
+    if (missing_frames) {
+      ON_SUCCEEDED(
+          sample_attrs->SetUINT32(MFSampleExtension_Discontinuity, TRUE));
+      hr = S_OK;
+    }
+  }
+
+  // Enqueue sample with Media Foundation
+  ON_SUCCEEDED(decoder_->ProcessInput(0, in_sample.Get(), 0));
+  return hr;
+}
+//
+// WebRTC-UWP End
+//
+
 int32_t H264DecoderImpl::Release() {
-  av_context_.reset();
-  av_frame_.reset();
+//  av_context_.reset();
+//  av_frame_.reset();
+//  return WEBRTC_VIDEO_CODEC_OK;
+
+  //
+  //  WebRTC-UWP Begin
+  //
+
+  OutputDebugString(L"WinUWPH264DecoderImpl::Release()\n");
+  HRESULT hr = S_OK;
+  inited_ = false;
+  
+  // Release I420 frame buffer pool
+  buffer_pool_.Release();
+  
+  if (decoder_ != NULL) {
+    // Follow shutdown procedure gracefully. On fail, continue anyway.
+    ON_SUCCEEDED(decoder_->ProcessMessage(MFT_MESSAGE_NOTIFY_END_OF_STREAM, 0));
+    ON_SUCCEEDED(decoder_->ProcessMessage(MFT_MESSAGE_COMMAND_DRAIN, NULL));
+    ON_SUCCEEDED(decoder_->ProcessMessage(MFT_MESSAGE_COMMAND_FLUSH, NULL));
+    decoder_ = nullptr;
+  }
+  
+  MFShutdown();
+  CoUninitialize();
+  
   return WEBRTC_VIDEO_CODEC_OK;
+
+  //
+  //  WebRTC-UWP End
+  //
 }
 
 int32_t H264DecoderImpl::RegisterDecodeCompleteCallback(
     DecodedImageCallback* callback) {
-  decoded_image_callback_ = callback;
+  //
+  //  WebRTC-UWP Begin
+  //
+  rtc::CritScope lock(&crit_);
+  //
+  //  WebRTC-UWP End
+  //
+  decode_complete_callback_ = callback;
   return WEBRTC_VIDEO_CODEC_OK;
 }
 
 int32_t H264DecoderImpl::Decode(const EncodedImage& input_image,
-                                bool /*missing_frames*/,
+                                // bool /*missing_frames*/,
+                                bool missing_frames,
                                 int64_t /*render_time_ms*/) {
-  if (!IsInitialized()) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
-  }
-  if (!decoded_image_callback_) {
+  //  if (!IsInitialized()) {
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+//  }
+  if (!decode_complete_callback_) {
     RTC_LOG(LS_WARNING)
         << "InitDecode() has been called, but a callback function "
            "has not been set with RegisterDecodeCompleteCallback()";
-    ReportError();
+    //ReportError();
     return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
   }
   if (!input_image.data() || !input_image.size()) {
-    ReportError();
+    //ReportError();
     return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
   }
-
-  AVPacket packet;
-  av_init_packet(&packet);
-  packet.data = input_image.mutable_data();
+//
+//  AVPacket packet;
+//  av_init_packet(&packet);
+//  packet.data = input_image.mutable_data();
   if (input_image.size() >
       static_cast<size_t>(std::numeric_limits<int>::max())) {
-    ReportError();
+    //ReportError();
     return WEBRTC_VIDEO_CODEC_ERROR;
   }
-  packet.size = static_cast<int>(input_image.size());
-  int64_t frame_timestamp_us = input_image.ntp_time_ms_ * 1000;  // ms -> μs
-  av_context_->reordered_opaque = frame_timestamp_us;
-
-  int result = avcodec_send_packet(av_context_.get(), &packet);
-  if (result < 0) {
-    RTC_LOG(LS_ERROR) << "avcodec_send_packet error: " << result;
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
+
+  //
+  // WebRTC-UWP Begin
+  //
+  HRESULT hr = S_OK;
+
+  if (!inited_) {
+    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
   }
 
-  result = avcodec_receive_frame(av_context_.get(), av_frame_.get());
-  if (result < 0) {
-    RTC_LOG(LS_ERROR) << "avcodec_receive_frame error: " << result;
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
+  if (decode_complete_callback_ == NULL) {
+    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
   }
 
-  // We don't expect reordering. Decoded frame tamestamp should match
-  // the input one.
-  RTC_DCHECK_EQ(av_frame_->reordered_opaque, frame_timestamp_us);
+  if (input_image.buffer() == NULL && input_image.size() > 0) {
+    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+  }
 
-  absl::optional<uint8_t> qp;
-  // TODO(sakal): Maybe it is possible to get QP directly from FFmpeg.
-  h264_bitstream_parser_.ParseBitstream(input_image.data(), input_image.size());
-  int qp_int;
-  if (h264_bitstream_parser_.GetLastSliceQp(&qp_int)) {
-    qp.emplace(qp_int);
+  // Discard until keyframe.
+  if (require_keyframe_) {
+    if (input_image._frameType != VideoFrameType::kVideoFrameKey ||
+        !input_image._completeFrame) {
+      return WEBRTC_VIDEO_CODEC_ERROR;
+    } else {
+      require_keyframe_ = false;
+    }
   }
 
-  // Obtain the |video_frame| containing the decoded image.
-  VideoFrame* input_frame =
-      static_cast<VideoFrame*>(av_buffer_get_opaque(av_frame_->buf[0]));
-  RTC_DCHECK(input_frame);
-  const webrtc::I420BufferInterface* i420_buffer =
-      input_frame->video_frame_buffer()->GetI420();
-
-  // When needed, FFmpeg applies cropping by moving plane pointers and adjusting
-  // frame width/height. Ensure that cropped buffers lie within the allocated
-  // memory.
-  RTC_DCHECK_LE(av_frame_->width, i420_buffer->width());
-  RTC_DCHECK_LE(av_frame_->height, i420_buffer->height());
-  RTC_DCHECK_GE(av_frame_->data[kYPlaneIndex], i420_buffer->DataY());
-  RTC_DCHECK_LE(
-      av_frame_->data[kYPlaneIndex] +
-          av_frame_->linesize[kYPlaneIndex] * av_frame_->height,
-      i420_buffer->DataY() + i420_buffer->StrideY() * i420_buffer->height());
-  RTC_DCHECK_GE(av_frame_->data[kUPlaneIndex], i420_buffer->DataU());
-  RTC_DCHECK_LE(av_frame_->data[kUPlaneIndex] +
-                    av_frame_->linesize[kUPlaneIndex] * av_frame_->height / 2,
-                i420_buffer->DataU() +
-                    i420_buffer->StrideU() * i420_buffer->height() / 2);
-  RTC_DCHECK_GE(av_frame_->data[kVPlaneIndex], i420_buffer->DataV());
-  RTC_DCHECK_LE(av_frame_->data[kVPlaneIndex] +
-                    av_frame_->linesize[kVPlaneIndex] * av_frame_->height / 2,
-                i420_buffer->DataV() +
-                    i420_buffer->StrideV() * i420_buffer->height() / 2);
-
-  auto cropped_buffer = WrapI420Buffer(
-      av_frame_->width, av_frame_->height, av_frame_->data[kYPlaneIndex],
-      av_frame_->linesize[kYPlaneIndex], av_frame_->data[kUPlaneIndex],
-      av_frame_->linesize[kUPlaneIndex], av_frame_->data[kVPlaneIndex],
-      av_frame_->linesize[kVPlaneIndex], rtc::KeepRefUntilDone(i420_buffer));
-
-  // Pass on color space from input frame if explicitly specified.
-  const ColorSpace& color_space =
-      input_image.ColorSpace() ? *input_image.ColorSpace()
-                               : ExtractH264ColorSpace(av_context_.get());
-
-  VideoFrame decoded_frame = VideoFrame::Builder()
-                                 .set_video_frame_buffer(cropped_buffer)
-                                 .set_timestamp_rtp(input_image.Timestamp())
-                                 .set_color_space(color_space)
-                                 .build();
-
-  // Return decoded frame.
-  // TODO(nisse): Timestamp and rotation are all zero here. Change decoder
-  // interface to pass a VideoFrameBuffer instead of a VideoFrame?
-  decoded_image_callback_->Decoded(decoded_frame, absl::nullopt, qp);
-
-  // Stop referencing it, possibly freeing |input_frame|.
-  av_frame_unref(av_frame_.get());
-  input_frame = nullptr;
+  // Enqueue the new frame with Media Foundation
+  ON_SUCCEEDED(EnqueueFrame(input_image, missing_frames));
+  if (hr == MF_E_NOTACCEPTING) {
+    // For robustness (shouldn't happen). Flush any old MF data blocking the
+    // new frames.
+    hr = decoder_->ProcessMessage(MFT_MESSAGE_COMMAND_FLUSH, NULL);
+
+    if (input_image._frameType == VideoFrameType::kVideoFrameKey) {
+      ON_SUCCEEDED(EnqueueFrame(input_image, missing_frames));
+    } else {
+      require_keyframe_ = true;
+      return WEBRTC_VIDEO_CODEC_ERROR;
+    }
+  }
 
-  return WEBRTC_VIDEO_CODEC_OK;
-}
+  if (FAILED(hr))
+    return WEBRTC_VIDEO_CODEC_ERROR;
 
-const char* H264DecoderImpl::ImplementationName() const {
-  return "FFmpeg";
-}
+  // Flush any decoded samples resulting from new frame, invoking callback
+  hr = FlushFrames(input_image.Timestamp(), input_image.ntp_time_ms_);
 
-bool H264DecoderImpl::IsInitialized() const {
-  return av_context_ != nullptr;
-}
+  if (hr == MF_E_TRANSFORM_STREAM_CHANGE) {
+    // Output media type is no longer suitable. Reconfigure and retry.
+    bool suitable_type_found;
+    hr = ConfigureOutputMediaType(decoder_, MFVideoFormat_NV12,
+                                  &suitable_type_found);
+
+    if (FAILED(hr) || !suitable_type_found)
+      return WEBRTC_VIDEO_CODEC_ERROR;
+
+    // Reset width and height in case output media size has changed (though it
+    // seems that would be unexpected, given that the input media would need to
+    // be manually changed too).
+    width_.reset();
+    height_.reset();
 
-void H264DecoderImpl::ReportInit() {
-  if (has_reported_init_)
-    return;
-  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264DecoderImpl.Event",
-                            kH264DecoderEventInit, kH264DecoderEventMax);
-  has_reported_init_ = true;
+    hr = FlushFrames(input_image.Timestamp(), input_image.ntp_time_ms_);
+  }
+
+  if (SUCCEEDED(hr) || hr == MF_E_TRANSFORM_NEED_MORE_INPUT) {
+    return WEBRTC_VIDEO_CODEC_OK;
+  }
+
+  return WEBRTC_VIDEO_CODEC_ERROR;
+  //
+  // WebRTC-UWP End
+  //
+
+
+//  packet.size = static_cast<int>(input_image.size());
+//  int64_t frame_timestamp_us = input_image.ntp_time_ms_ * 1000;  // ms -> μs
+//  av_context_->reordered_opaque = frame_timestamp_us;
+//
+//  int result = avcodec_send_packet(av_context_.get(), &packet);
+//  if (result < 0) {
+//    RTC_LOG(LS_ERROR) << "avcodec_send_packet error: " << result;
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERROR;
+//  }
+//
+//  result = avcodec_receive_frame(av_context_.get(), av_frame_.get());
+//  if (result < 0) {
+//    RTC_LOG(LS_ERROR) << "avcodec_receive_frame error: " << result;
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERROR;
+//  }
+//
+//  // We don't expect reordering. Decoded frame tamestamp should match
+//  // the input one.
+//  RTC_DCHECK_EQ(av_frame_->reordered_opaque, frame_timestamp_us);
+//
+//  absl::optional<uint8_t> qp;
+//  // TODO(sakal): Maybe it is possible to get QP directly from FFmpeg.
+//  h264_bitstream_parser_.ParseBitstream(input_image.data(), input_image.size());
+//  int qp_int;
+//  if (h264_bitstream_parser_.GetLastSliceQp(&qp_int)) {
+//    qp.emplace(qp_int);
+//  }
+//
+//  // Obtain the |video_frame| containing the decoded image.
+//  VideoFrame* input_frame =
+//      static_cast<VideoFrame*>(av_buffer_get_opaque(av_frame_->buf[0]));
+//  RTC_DCHECK(input_frame);
+//  const webrtc::I420BufferInterface* i420_buffer =
+//      input_frame->video_frame_buffer()->GetI420();
+//
+//  // When needed, FFmpeg applies cropping by moving plane pointers and adjusting
+//  // frame width/height. Ensure that cropped buffers lie within the allocated
+//  // memory.
+//  RTC_DCHECK_LE(av_frame_->width, i420_buffer->width());
+//  RTC_DCHECK_LE(av_frame_->height, i420_buffer->height());
+//  RTC_DCHECK_GE(av_frame_->data[kYPlaneIndex], i420_buffer->DataY());
+//  RTC_DCHECK_LE(
+//      av_frame_->data[kYPlaneIndex] +
+//          av_frame_->linesize[kYPlaneIndex] * av_frame_->height,
+//      i420_buffer->DataY() + i420_buffer->StrideY() * i420_buffer->height());
+//  RTC_DCHECK_GE(av_frame_->data[kUPlaneIndex], i420_buffer->DataU());
+//  RTC_DCHECK_LE(av_frame_->data[kUPlaneIndex] +
+//                    av_frame_->linesize[kUPlaneIndex] * av_frame_->height / 2,
+//                i420_buffer->DataU() +
+//                    i420_buffer->StrideU() * i420_buffer->height() / 2);
+//  RTC_DCHECK_GE(av_frame_->data[kVPlaneIndex], i420_buffer->DataV());
+//  RTC_DCHECK_LE(av_frame_->data[kVPlaneIndex] +
+//                    av_frame_->linesize[kVPlaneIndex] * av_frame_->height / 2,
+//                i420_buffer->DataV() +
+//                    i420_buffer->StrideV() * i420_buffer->height() / 2);
+//
+//  auto cropped_buffer = WrapI420Buffer(
+//      av_frame_->width, av_frame_->height, av_frame_->data[kYPlaneIndex],
+//      av_frame_->linesize[kYPlaneIndex], av_frame_->data[kUPlaneIndex],
+//      av_frame_->linesize[kUPlaneIndex], av_frame_->data[kVPlaneIndex],
+//      av_frame_->linesize[kVPlaneIndex], rtc::KeepRefUntilDone(i420_buffer));
+//
+//  // Pass on color space from input frame if explicitly specified.
+//  const ColorSpace& color_space =
+//      input_image.ColorSpace() ? *input_image.ColorSpace()
+//                               : ExtractH264ColorSpace(av_context_.get());
+//
+//  VideoFrame decoded_frame = VideoFrame::Builder()
+//                                 .set_video_frame_buffer(cropped_buffer)
+//                                 .set_timestamp_rtp(input_image.Timestamp())
+//                                 .set_color_space(color_space)
+//                                 .build();
+//
+//  // Return decoded frame.
+//  // TODO(nisse): Timestamp and rotation are all zero here. Change decoder
+//  // interface to pass a VideoFrameBuffer instead of a VideoFrame?
+//  decoded_image_callback_->Decoded(decoded_frame, absl::nullopt, qp);
+//
+//  // Stop referencing it, possibly freeing |input_frame|.
+//  av_frame_unref(av_frame_.get());
+//  input_frame = nullptr;
+
+//  return WEBRTC_VIDEO_CODEC_OK;
 }
 
-void H264DecoderImpl::ReportError() {
-  if (has_reported_error_)
-    return;
-  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264DecoderImpl.Event",
-                            kH264DecoderEventError, kH264DecoderEventMax);
-  has_reported_error_ = true;
+const char* H264DecoderImpl::ImplementationName() const {
+  //return "FFmpeg";
+  return "H264_MediaFoundation";
 }
 
+//bool H264DecoderImpl::IsInitialized() const {
+//  return false; // av_context_ != nullptr;
+//}
+
+//void H264DecoderImpl::ReportInit() {
+//  if (has_reported_init_)
+//    return;
+//  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264DecoderImpl.Event",
+//                            kH264DecoderEventInit, kH264DecoderEventMax);
+//  has_reported_init_ = true;
+//}
+//
+//void H264DecoderImpl::ReportError() {
+//  if (has_reported_error_)
+//    return;
+//  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264DecoderImpl.Event",
+//                            kH264DecoderEventError, kH264DecoderEventMax);
+//  has_reported_error_ = true;
+//}
+
 }  // namespace webrtc
 
 #endif  // WEBRTC_USE_H264
diff --git a/modules/video_coding/codecs/h264/h264_decoder_impl.h b/modules/video_coding/codecs/h264/h264_decoder_impl.h
index 3c038e6425..4a1c416b83 100644
--- a/modules/video_coding/codecs/h264/h264_decoder_impl.h
+++ b/modules/video_coding/codecs/h264/h264_decoder_impl.h
@@ -17,12 +17,30 @@
 // #ifdef unless needed and tested.
 #ifdef WEBRTC_USE_H264
 
-#if defined(WEBRTC_WIN) && !defined(__clang__)
-#error "See: bugs.webrtc.org/9213#c13."
-#endif
+//#if defined(WEBRTC_WIN) && !defined(__clang__)
+//#error "See: bugs.webrtc.org/9213#c13."
+//#endif
 
 #include <memory>
 
+//
+// WebRTC-UWP Begin
+//
+#include <mfapi.h>
+#include <mfidl.h>
+#include <Mfreadwrite.h>
+#include <mferror.h>
+#include <wrl.h>
+
+#include "rtc_base/critical_section.h"
+
+#pragma comment(lib, "mfreadwrite")
+#pragma comment(lib, "mfplat")
+#pragma comment(lib, "mfuuid")
+//
+// WebRTC-UWP End
+//
+
 #include "modules/video_coding/codecs/h264/include/h264.h"
 
 // CAVEAT: According to ffmpeg docs for avcodec_send_packet, ffmpeg requires a
@@ -39,21 +57,21 @@
 // have to add an extra copy operation, to enforce padding before buffers are
 // passed to ffmpeg.
 
-extern "C" {
-#include "third_party/ffmpeg/libavcodec/avcodec.h"
-}  // extern "C"
+//extern "C" {
+//#include "third_party/ffmpeg/libavcodec/avcodec.h"
+//}  // extern "C"
 
-#include "common_video/h264/h264_bitstream_parser.h"
+// #include "common_video/h264/h264_bitstream_parser.h"
 #include "common_video/include/i420_buffer_pool.h"
 
 namespace webrtc {
 
-struct AVCodecContextDeleter {
-  void operator()(AVCodecContext* ptr) const { avcodec_free_context(&ptr); }
-};
-struct AVFrameDeleter {
-  void operator()(AVFrame* ptr) const { av_frame_free(&ptr); }
-};
+//struct AVCodecContextDeleter {
+//  void operator()(AVCodecContext* ptr) const { avcodec_free_context(&ptr); }
+//};
+//struct AVFrameDeleter {
+//  void operator()(AVFrame* ptr) const { av_frame_free(&ptr); }
+//};
 
 class H264DecoderImpl : public H264Decoder {
  public:
@@ -77,31 +95,52 @@ class H264DecoderImpl : public H264Decoder {
   const char* ImplementationName() const override;
 
  private:
+   //
+   // WebRTC-UWP Begin
+   //
+  HRESULT FlushFrames(uint32_t timestamp, uint64_t ntp_time_ms);
+  HRESULT EnqueueFrame(const EncodedImage& input_image, bool missing_frames);
+
+ private:
+  Microsoft::WRL::ComPtr<IMFTransform> decoder_;
+  I420BufferPool buffer_pool_;
+
+  bool inited_ = false;
+  bool require_keyframe_ = true;
+  uint32_t first_frame_rtp_ = 0;
+  absl::optional<uint32_t> width_;
+  absl::optional<uint32_t> height_;
+  rtc::CriticalSection crit_;
+  DecodedImageCallback* decode_complete_callback_;
+  //
+  // WebRTC-UWP End
+  //
+
   // Called by FFmpeg when it needs a frame buffer to store decoded frames in.
   // The |VideoFrame| returned by FFmpeg at |Decode| originate from here. Their
   // buffers are reference counted and freed by FFmpeg using |AVFreeBuffer2|.
-  static int AVGetBuffer2(AVCodecContext* context,
-                          AVFrame* av_frame,
-                          int flags);
+//  static int AVGetBuffer2(AVCodecContext* context,
+//                          AVFrame* av_frame,
+//                          int flags);
   // Called by FFmpeg when it is done with a video frame, see |AVGetBuffer2|.
-  static void AVFreeBuffer2(void* opaque, uint8_t* data);
+  // static void AVFreeBuffer2(void* opaque, uint8_t* data);
 
-  bool IsInitialized() const;
+  // bool IsInitialized() const;
 
   // Reports statistics with histograms.
-  void ReportInit();
-  void ReportError();
+  // void ReportInit();
+  // void ReportError();
 
-  I420BufferPool pool_;
-  std::unique_ptr<AVCodecContext, AVCodecContextDeleter> av_context_;
-  std::unique_ptr<AVFrame, AVFrameDeleter> av_frame_;
+  // I420BufferPool pool_;
+  //std::unique_ptr<AVCodecContext, AVCodecContextDeleter> av_context_;
+  //std::unique_ptr<AVFrame, AVFrameDeleter> av_frame_;
 
-  DecodedImageCallback* decoded_image_callback_;
+  //DecodedImageCallback* decoded_image_callback_;
 
-  bool has_reported_init_;
-  bool has_reported_error_;
+  // bool has_reported_init_;
+  // bool has_reported_error_;
 
-  webrtc::H264BitstreamParser h264_bitstream_parser_;
+  // webrtc::H264BitstreamParser h264_bitstream_parser_;
 };
 
 }  // namespace webrtc
diff --git a/modules/video_coding/codecs/h264/h264_encoder_impl.cc b/modules/video_coding/codecs/h264/h264_encoder_impl.cc
index 66861e6e74..55909c0d12 100644
--- a/modules/video_coding/codecs/h264/h264_encoder_impl.cc
+++ b/modules/video_coding/codecs/h264/h264_encoder_impl.cc
@@ -29,10 +29,10 @@
 #include "system_wrappers/include/metrics.h"
 #include "third_party/libyuv/include/libyuv/convert.h"
 #include "third_party/libyuv/include/libyuv/scale.h"
-#include "third_party/openh264/src/codec/api/svc/codec_api.h"
-#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
-#include "third_party/openh264/src/codec/api/svc/codec_def.h"
-#include "third_party/openh264/src/codec/api/svc/codec_ver.h"
+//#include "third_party/openh264/src/codec/api/svc/codec_api.h"
+//#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
+//#include "third_party/openh264/src/codec/api/svc/codec_def.h"
+//#include "third_party/openh264/src/codec/api/svc/codec_ver.h"
 
 namespace webrtc {
 
@@ -68,21 +68,21 @@ int NumberOfThreads(int width, int height, int number_of_cores) {
   return 1;
 }
 
-VideoFrameType ConvertToVideoFrameType(EVideoFrameType type) {
-  switch (type) {
-    case videoFrameTypeIDR:
-      return VideoFrameType::kVideoFrameKey;
-    case videoFrameTypeSkip:
-    case videoFrameTypeI:
-    case videoFrameTypeP:
-    case videoFrameTypeIPMixed:
-      return VideoFrameType::kVideoFrameDelta;
-    case videoFrameTypeInvalid:
-      break;
-  }
-  RTC_NOTREACHED() << "Unexpected/invalid frame type: " << type;
-  return VideoFrameType::kEmptyFrame;
-}
+//VideoFrameType ConvertToVideoFrameType(EVideoFrameType type) {
+//  switch (type) {
+//    case videoFrameTypeIDR:
+//      return VideoFrameType::kVideoFrameKey;
+//    case videoFrameTypeSkip:
+//    case videoFrameTypeI:
+//    case videoFrameTypeP:
+//    case videoFrameTypeIPMixed:
+//      return VideoFrameType::kVideoFrameDelta;
+//    case videoFrameTypeInvalid:
+//      break;
+//  }
+//  RTC_NOTREACHED() << "Unexpected/invalid frame type: " << type;
+//  return VideoFrameType::kEmptyFrame;
+//}
 
 }  // namespace
 
@@ -97,56 +97,56 @@ VideoFrameType ConvertToVideoFrameType(EVideoFrameType type) {
 // start codes) is copied to the |encoded_image->_buffer| and the |frag_header|
 // is updated to point to each fragment, with offsets and lengths set as to
 // exclude the start codes.
-static void RtpFragmentize(EncodedImage* encoded_image,
-                           const VideoFrameBuffer& frame_buffer,
-                           SFrameBSInfo* info,
-                           RTPFragmentationHeader* frag_header) {
-  // Calculate minimum buffer size required to hold encoded data.
-  size_t required_capacity = 0;
-  size_t fragments_count = 0;
-  for (int layer = 0; layer < info->iLayerNum; ++layer) {
-    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
-    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++fragments_count) {
-      RTC_CHECK_GE(layerInfo.pNalLengthInByte[nal], 0);
-      // Ensure |required_capacity| will not overflow.
-      RTC_CHECK_LE(layerInfo.pNalLengthInByte[nal],
-                   std::numeric_limits<size_t>::max() - required_capacity);
-      required_capacity += layerInfo.pNalLengthInByte[nal];
-    }
-  }
-  // TODO(nisse): Use a cache or buffer pool to avoid allocation?
-  encoded_image->SetEncodedData(EncodedImageBuffer::Create(required_capacity));
-
-  // Iterate layers and NAL units, note each NAL unit as a fragment and copy
-  // the data to |encoded_image->_buffer|.
-  const uint8_t start_code[4] = {0, 0, 0, 1};
-  frag_header->VerifyAndAllocateFragmentationHeader(fragments_count);
-  size_t frag = 0;
-  encoded_image->set_size(0);
-  for (int layer = 0; layer < info->iLayerNum; ++layer) {
-    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
-    // Iterate NAL units making up this layer, noting fragments.
-    size_t layer_len = 0;
-    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++frag) {
-      // Because the sum of all layer lengths, |required_capacity|, fits in a
-      // |size_t|, we know that any indices in-between will not overflow.
-      RTC_DCHECK_GE(layerInfo.pNalLengthInByte[nal], 4);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 0], start_code[0]);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 1], start_code[1]);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 2], start_code[2]);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 3], start_code[3]);
-      frag_header->fragmentationOffset[frag] =
-          encoded_image->size() + layer_len + sizeof(start_code);
-      frag_header->fragmentationLength[frag] =
-          layerInfo.pNalLengthInByte[nal] - sizeof(start_code);
-      layer_len += layerInfo.pNalLengthInByte[nal];
-    }
-    // Copy the entire layer's data (including start codes).
-    memcpy(encoded_image->data() + encoded_image->size(), layerInfo.pBsBuf,
-           layer_len);
-    encoded_image->set_size(encoded_image->size() + layer_len);
-  }
-}
+//static void RtpFragmentize(EncodedImage* encoded_image,
+//                           const VideoFrameBuffer& frame_buffer,
+//                           SFrameBSInfo* info,
+//                           RTPFragmentationHeader* frag_header) {
+//  // Calculate minimum buffer size required to hold encoded data.
+//  size_t required_capacity = 0;
+//  size_t fragments_count = 0;
+//  for (int layer = 0; layer < info->iLayerNum; ++layer) {
+//    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
+//    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++fragments_count) {
+//      RTC_CHECK_GE(layerInfo.pNalLengthInByte[nal], 0);
+//      // Ensure |required_capacity| will not overflow.
+//      RTC_CHECK_LE(layerInfo.pNalLengthInByte[nal],
+//                   std::numeric_limits<size_t>::max() - required_capacity);
+//      required_capacity += layerInfo.pNalLengthInByte[nal];
+//    }
+//  }
+//  // TODO(nisse): Use a cache or buffer pool to avoid allocation?
+//  encoded_image->SetEncodedData(EncodedImageBuffer::Create(required_capacity));
+//
+//  // Iterate layers and NAL units, note each NAL unit as a fragment and copy
+//  // the data to |encoded_image->_buffer|.
+//  const uint8_t start_code[4] = {0, 0, 0, 1};
+//  frag_header->VerifyAndAllocateFragmentationHeader(fragments_count);
+//  size_t frag = 0;
+//  encoded_image->set_size(0);
+//  for (int layer = 0; layer < info->iLayerNum; ++layer) {
+//    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
+//    // Iterate NAL units making up this layer, noting fragments.
+//    size_t layer_len = 0;
+//    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++frag) {
+//      // Because the sum of all layer lengths, |required_capacity|, fits in a
+//      // |size_t|, we know that any indices in-between will not overflow.
+//      RTC_DCHECK_GE(layerInfo.pNalLengthInByte[nal], 4);
+//      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 0], start_code[0]);
+//      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 1], start_code[1]);
+//      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 2], start_code[2]);
+//      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len + 3], start_code[3]);
+//      frag_header->fragmentationOffset[frag] =
+//          encoded_image->size() + layer_len + sizeof(start_code);
+//      frag_header->fragmentationLength[frag] =
+//          layerInfo.pNalLengthInByte[nal] - sizeof(start_code);
+//      layer_len += layerInfo.pNalLengthInByte[nal];
+//    }
+//    // Copy the entire layer's data (including start codes).
+//    memcpy(encoded_image->data() + encoded_image->size(), layerInfo.pBsBuf,
+//           layer_len);
+//    encoded_image->set_size(encoded_image->size() + layer_len);
+//  }
+//}
 
 H264EncoderImpl::H264EncoderImpl(const cricket::VideoCodec& codec)
     : packetization_mode_(H264PacketizationMode::SingleNalUnit),
@@ -155,18 +155,18 @@ H264EncoderImpl::H264EncoderImpl(const cricket::VideoCodec& codec)
       encoded_image_callback_(nullptr),
       has_reported_init_(false),
       has_reported_error_(false) {
-  RTC_CHECK(absl::EqualsIgnoreCase(codec.name, cricket::kH264CodecName));
-  std::string packetization_mode_string;
-  if (codec.GetParam(cricket::kH264FmtpPacketizationMode,
-                     &packetization_mode_string) &&
-      packetization_mode_string == "1") {
-    packetization_mode_ = H264PacketizationMode::NonInterleaved;
-  }
-  downscaled_buffers_.reserve(kMaxSimulcastStreams - 1);
-  encoded_images_.reserve(kMaxSimulcastStreams);
-  encoders_.reserve(kMaxSimulcastStreams);
-  configurations_.reserve(kMaxSimulcastStreams);
-  tl0sync_limit_.reserve(kMaxSimulcastStreams);
+//  RTC_CHECK(absl::EqualsIgnoreCase(codec.name, cricket::kH264CodecName));
+//  std::string packetization_mode_string;
+//  if (codec.GetParam(cricket::kH264FmtpPacketizationMode,
+//                     &packetization_mode_string) &&
+//      packetization_mode_string == "1") {
+//    packetization_mode_ = H264PacketizationMode::NonInterleaved;
+//  }
+//  downscaled_buffers_.reserve(kMaxSimulcastStreams - 1);
+//  encoded_images_.reserve(kMaxSimulcastStreams);
+//  encoders_.reserve(kMaxSimulcastStreams);
+//  configurations_.reserve(kMaxSimulcastStreams);
+//  tl0sync_limit_.reserve(kMaxSimulcastStreams);
 }
 
 H264EncoderImpl::~H264EncoderImpl() {
@@ -175,353 +175,353 @@ H264EncoderImpl::~H264EncoderImpl() {
 
 int32_t H264EncoderImpl::InitEncode(const VideoCodec* inst,
                                     const VideoEncoder::Settings& settings) {
-  ReportInit();
-  if (!inst || inst->codecType != kVideoCodecH264) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-  }
-  if (inst->maxFramerate == 0) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-  }
-  if (inst->width < 1 || inst->height < 1) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-  }
-
-  int32_t release_ret = Release();
-  if (release_ret != WEBRTC_VIDEO_CODEC_OK) {
-    ReportError();
-    return release_ret;
-  }
-
-  int number_of_streams = SimulcastUtility::NumberOfSimulcastStreams(*inst);
-  bool doing_simulcast = (number_of_streams > 1);
-
-  if (doing_simulcast &&
-      !SimulcastUtility::ValidSimulcastParameters(*inst, number_of_streams)) {
-    return WEBRTC_VIDEO_CODEC_ERR_SIMULCAST_PARAMETERS_NOT_SUPPORTED;
-  }
-  downscaled_buffers_.resize(number_of_streams - 1);
-  encoded_images_.resize(number_of_streams);
-  encoders_.resize(number_of_streams);
-  pictures_.resize(number_of_streams);
-  configurations_.resize(number_of_streams);
-  tl0sync_limit_.resize(number_of_streams);
-
-  number_of_cores_ = settings.number_of_cores;
-  max_payload_size_ = settings.max_payload_size;
-  codec_ = *inst;
-
-  // Code expects simulcastStream resolutions to be correct, make sure they are
-  // filled even when there are no simulcast layers.
-  if (codec_.numberOfSimulcastStreams == 0) {
-    codec_.simulcastStream[0].width = codec_.width;
-    codec_.simulcastStream[0].height = codec_.height;
-  }
-
-  for (int i = 0, idx = number_of_streams - 1; i < number_of_streams;
-       ++i, --idx) {
-    ISVCEncoder* openh264_encoder;
-    // Create encoder.
-    if (WelsCreateSVCEncoder(&openh264_encoder) != 0) {
-      // Failed to create encoder.
-      RTC_LOG(LS_ERROR) << "Failed to create OpenH264 encoder";
-      RTC_DCHECK(!openh264_encoder);
-      Release();
-      ReportError();
-      return WEBRTC_VIDEO_CODEC_ERROR;
-    }
-    RTC_DCHECK(openh264_encoder);
-    if (kOpenH264EncoderDetailedLogging) {
-      int trace_level = WELS_LOG_DETAIL;
-      openh264_encoder->SetOption(ENCODER_OPTION_TRACE_LEVEL, &trace_level);
-    }
-    // else WELS_LOG_DEFAULT is used by default.
-
-    // Store h264 encoder.
-    encoders_[i] = openh264_encoder;
-
-    // Set internal settings from codec_settings
-    configurations_[i].simulcast_idx = idx;
-    configurations_[i].sending = false;
-    configurations_[i].width = codec_.simulcastStream[idx].width;
-    configurations_[i].height = codec_.simulcastStream[idx].height;
-    configurations_[i].max_frame_rate = static_cast<float>(codec_.maxFramerate);
-    configurations_[i].frame_dropping_on = codec_.H264()->frameDroppingOn;
-    configurations_[i].key_frame_interval = codec_.H264()->keyFrameInterval;
-    configurations_[i].num_temporal_layers =
-        codec_.simulcastStream[idx].numberOfTemporalLayers;
-
-    // Create downscaled image buffers.
-    if (i > 0) {
-      downscaled_buffers_[i - 1] = I420Buffer::Create(
-          configurations_[i].width, configurations_[i].height,
-          configurations_[i].width, configurations_[i].width / 2,
-          configurations_[i].width / 2);
-    }
-
-    // Codec_settings uses kbits/second; encoder uses bits/second.
-    configurations_[i].max_bps = codec_.maxBitrate * 1000;
-    configurations_[i].target_bps = codec_.startBitrate * 1000;
-
-    // Create encoder parameters based on the layer configuration.
-    SEncParamExt encoder_params = CreateEncoderParams(i);
-
-    // Initialize.
-    if (openh264_encoder->InitializeExt(&encoder_params) != 0) {
-      RTC_LOG(LS_ERROR) << "Failed to initialize OpenH264 encoder";
-      Release();
-      ReportError();
-      return WEBRTC_VIDEO_CODEC_ERROR;
-    }
-    // TODO(pbos): Base init params on these values before submitting.
-    int video_format = EVideoFormatType::videoFormatI420;
-    openh264_encoder->SetOption(ENCODER_OPTION_DATAFORMAT, &video_format);
-
-    // Initialize encoded image. Default buffer size: size of unencoded data.
-
-    const size_t new_capacity =
-        CalcBufferSize(VideoType::kI420, codec_.simulcastStream[idx].width,
-                       codec_.simulcastStream[idx].height);
-    encoded_images_[i].SetEncodedData(EncodedImageBuffer::Create(new_capacity));
-    encoded_images_[i]._completeFrame = true;
-    encoded_images_[i]._encodedWidth = codec_.simulcastStream[idx].width;
-    encoded_images_[i]._encodedHeight = codec_.simulcastStream[idx].height;
-    encoded_images_[i].set_size(0);
-
-    tl0sync_limit_[i] = configurations_[i].num_temporal_layers;
-  }
-
-  SimulcastRateAllocator init_allocator(codec_);
-  VideoBitrateAllocation allocation =
-      init_allocator.Allocate(VideoBitrateAllocationParameters(
-          DataRate::kbps(codec_.startBitrate), codec_.maxFramerate));
-  SetRates(RateControlParameters(allocation, codec_.maxFramerate));
+//  ReportInit();
+//  if (!inst || inst->codecType != kVideoCodecH264) {
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+//  }
+//  if (inst->maxFramerate == 0) {
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+//  }
+//  if (inst->width < 1 || inst->height < 1) {
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+//  }
+//
+//  int32_t release_ret = Release();
+//  if (release_ret != WEBRTC_VIDEO_CODEC_OK) {
+//    ReportError();
+//    return release_ret;
+//  }
+//
+//  int number_of_streams = SimulcastUtility::NumberOfSimulcastStreams(*inst);
+//  bool doing_simulcast = (number_of_streams > 1);
+//
+//  if (doing_simulcast &&
+//      !SimulcastUtility::ValidSimulcastParameters(*inst, number_of_streams)) {
+//    return WEBRTC_VIDEO_CODEC_ERR_SIMULCAST_PARAMETERS_NOT_SUPPORTED;
+//  }
+//  downscaled_buffers_.resize(number_of_streams - 1);
+//  encoded_images_.resize(number_of_streams);
+//  encoders_.resize(number_of_streams);
+//  pictures_.resize(number_of_streams);
+//  configurations_.resize(number_of_streams);
+//  tl0sync_limit_.resize(number_of_streams);
+//
+//  number_of_cores_ = settings.number_of_cores;
+//  max_payload_size_ = settings.max_payload_size;
+//  codec_ = *inst;
+//
+//  // Code expects simulcastStream resolutions to be correct, make sure they are
+//  // filled even when there are no simulcast layers.
+//  if (codec_.numberOfSimulcastStreams == 0) {
+//    codec_.simulcastStream[0].width = codec_.width;
+//    codec_.simulcastStream[0].height = codec_.height;
+//  }
+//
+//  for (int i = 0, idx = number_of_streams - 1; i < number_of_streams;
+//       ++i, --idx) {
+//    ISVCEncoder* openh264_encoder;
+//    // Create encoder.
+//    if (WelsCreateSVCEncoder(&openh264_encoder) != 0) {
+//      // Failed to create encoder.
+//      RTC_LOG(LS_ERROR) << "Failed to create OpenH264 encoder";
+//      RTC_DCHECK(!openh264_encoder);
+//      Release();
+//      ReportError();
+//      return WEBRTC_VIDEO_CODEC_ERROR;
+//    }
+//    RTC_DCHECK(openh264_encoder);
+//    if (kOpenH264EncoderDetailedLogging) {
+//      int trace_level = WELS_LOG_DETAIL;
+//      openh264_encoder->SetOption(ENCODER_OPTION_TRACE_LEVEL, &trace_level);
+//    }
+//    // else WELS_LOG_DEFAULT is used by default.
+//
+//    // Store h264 encoder.
+//    encoders_[i] = openh264_encoder;
+//
+//    // Set internal settings from codec_settings
+//    configurations_[i].simulcast_idx = idx;
+//    configurations_[i].sending = false;
+//    configurations_[i].width = codec_.simulcastStream[idx].width;
+//    configurations_[i].height = codec_.simulcastStream[idx].height;
+//    configurations_[i].max_frame_rate = static_cast<float>(codec_.maxFramerate);
+//    configurations_[i].frame_dropping_on = codec_.H264()->frameDroppingOn;
+//    configurations_[i].key_frame_interval = codec_.H264()->keyFrameInterval;
+//    configurations_[i].num_temporal_layers =
+//        codec_.simulcastStream[idx].numberOfTemporalLayers;
+//
+//    // Create downscaled image buffers.
+//    if (i > 0) {
+//      downscaled_buffers_[i - 1] = I420Buffer::Create(
+//          configurations_[i].width, configurations_[i].height,
+//          configurations_[i].width, configurations_[i].width / 2,
+//          configurations_[i].width / 2);
+//    }
+//
+//    // Codec_settings uses kbits/second; encoder uses bits/second.
+//    configurations_[i].max_bps = codec_.maxBitrate * 1000;
+//    configurations_[i].target_bps = codec_.startBitrate * 1000;
+//
+//    // Create encoder parameters based on the layer configuration.
+//    SEncParamExt encoder_params = CreateEncoderParams(i);
+//
+//    // Initialize.
+//    if (openh264_encoder->InitializeExt(&encoder_params) != 0) {
+//      RTC_LOG(LS_ERROR) << "Failed to initialize OpenH264 encoder";
+//      Release();
+//      ReportError();
+//      return WEBRTC_VIDEO_CODEC_ERROR;
+//    }
+//    // TODO(pbos): Base init params on these values before submitting.
+//    int video_format = EVideoFormatType::videoFormatI420;
+//    openh264_encoder->SetOption(ENCODER_OPTION_DATAFORMAT, &video_format);
+//
+//    // Initialize encoded image. Default buffer size: size of unencoded data.
+//
+//    const size_t new_capacity =
+//        CalcBufferSize(VideoType::kI420, codec_.simulcastStream[idx].width,
+//                       codec_.simulcastStream[idx].height);
+//    encoded_images_[i].SetEncodedData(EncodedImageBuffer::Create(new_capacity));
+//    encoded_images_[i]._completeFrame = true;
+//    encoded_images_[i]._encodedWidth = codec_.simulcastStream[idx].width;
+//    encoded_images_[i]._encodedHeight = codec_.simulcastStream[idx].height;
+//    encoded_images_[i].set_size(0);
+//
+//    tl0sync_limit_[i] = configurations_[i].num_temporal_layers;
+//  }
+//
+//  SimulcastRateAllocator init_allocator(codec_);
+//  VideoBitrateAllocation allocation =
+//      init_allocator.Allocate(VideoBitrateAllocationParameters(
+//          DataRate::kbps(codec_.startBitrate), codec_.maxFramerate));
+//  SetRates(RateControlParameters(allocation, codec_.maxFramerate));
   return WEBRTC_VIDEO_CODEC_OK;
 }
 
 int32_t H264EncoderImpl::Release() {
-  while (!encoders_.empty()) {
-    ISVCEncoder* openh264_encoder = encoders_.back();
-    if (openh264_encoder) {
-      RTC_CHECK_EQ(0, openh264_encoder->Uninitialize());
-      WelsDestroySVCEncoder(openh264_encoder);
-    }
-    encoders_.pop_back();
-  }
-  downscaled_buffers_.clear();
-  configurations_.clear();
-  encoded_images_.clear();
-  pictures_.clear();
-  tl0sync_limit_.clear();
+//  while (!encoders_.empty()) {
+//    ISVCEncoder* openh264_encoder = encoders_.back();
+//    if (openh264_encoder) {
+//      RTC_CHECK_EQ(0, openh264_encoder->Uninitialize());
+//      WelsDestroySVCEncoder(openh264_encoder);
+//    }
+//    encoders_.pop_back();
+//  }
+//  downscaled_buffers_.clear();
+//  configurations_.clear();
+//  encoded_images_.clear();
+//  pictures_.clear();
+//  tl0sync_limit_.clear();
   return WEBRTC_VIDEO_CODEC_OK;
 }
 
 int32_t H264EncoderImpl::RegisterEncodeCompleteCallback(
     EncodedImageCallback* callback) {
-  encoded_image_callback_ = callback;
+//  encoded_image_callback_ = callback;
   return WEBRTC_VIDEO_CODEC_OK;
 }
 
 void H264EncoderImpl::SetRates(const RateControlParameters& parameters) {
-  if (encoders_.empty()) {
-    RTC_LOG(LS_WARNING) << "SetRates() while uninitialized.";
-    return;
-  }
-
-  if (parameters.framerate_fps < 1.0) {
-    RTC_LOG(LS_WARNING) << "Invalid frame rate: " << parameters.framerate_fps;
-    return;
-  }
-
-  if (parameters.bitrate.get_sum_bps() == 0) {
-    // Encoder paused, turn off all encoding.
-    for (size_t i = 0; i < configurations_.size(); ++i) {
-      configurations_[i].SetStreamState(false);
-    }
-    return;
-  }
-
-  codec_.maxFramerate = static_cast<uint32_t>(parameters.framerate_fps);
-
-  size_t stream_idx = encoders_.size() - 1;
-  for (size_t i = 0; i < encoders_.size(); ++i, --stream_idx) {
-    // Update layer config.
-    configurations_[i].target_bps =
-        parameters.bitrate.GetSpatialLayerSum(stream_idx);
-    configurations_[i].max_frame_rate = parameters.framerate_fps;
-
-    if (configurations_[i].target_bps) {
-      configurations_[i].SetStreamState(true);
-
-      // Update h264 encoder.
-      SBitrateInfo target_bitrate;
-      memset(&target_bitrate, 0, sizeof(SBitrateInfo));
-      target_bitrate.iLayer = SPATIAL_LAYER_ALL,
-      target_bitrate.iBitrate = configurations_[i].target_bps;
-      encoders_[i]->SetOption(ENCODER_OPTION_BITRATE, &target_bitrate);
-      encoders_[i]->SetOption(ENCODER_OPTION_FRAME_RATE,
-                              &configurations_[i].max_frame_rate);
-    } else {
-      configurations_[i].SetStreamState(false);
-    }
-  }
+//  if (encoders_.empty()) {
+//    RTC_LOG(LS_WARNING) << "SetRates() while uninitialized.";
+//    return;
+//  }
+//
+//  if (parameters.framerate_fps < 1.0) {
+//    RTC_LOG(LS_WARNING) << "Invalid frame rate: " << parameters.framerate_fps;
+//    return;
+//  }
+//
+//  if (parameters.bitrate.get_sum_bps() == 0) {
+//    // Encoder paused, turn off all encoding.
+//    for (size_t i = 0; i < configurations_.size(); ++i) {
+//      configurations_[i].SetStreamState(false);
+//    }
+//    return;
+//  }
+//
+//  codec_.maxFramerate = static_cast<uint32_t>(parameters.framerate_fps);
+//
+//  size_t stream_idx = encoders_.size() - 1;
+//  for (size_t i = 0; i < encoders_.size(); ++i, --stream_idx) {
+//    // Update layer config.
+//    configurations_[i].target_bps =
+//        parameters.bitrate.GetSpatialLayerSum(stream_idx);
+//    configurations_[i].max_frame_rate = parameters.framerate_fps;
+//
+//    if (configurations_[i].target_bps) {
+//      configurations_[i].SetStreamState(true);
+//
+//      // Update h264 encoder.
+//      SBitrateInfo target_bitrate;
+//      memset(&target_bitrate, 0, sizeof(SBitrateInfo));
+//      target_bitrate.iLayer = SPATIAL_LAYER_ALL,
+//      target_bitrate.iBitrate = configurations_[i].target_bps;
+//      encoders_[i]->SetOption(ENCODER_OPTION_BITRATE, &target_bitrate);
+//      encoders_[i]->SetOption(ENCODER_OPTION_FRAME_RATE,
+//                              &configurations_[i].max_frame_rate);
+//    } else {
+//      configurations_[i].SetStreamState(false);
+//    }
+//  }
 }
 
 int32_t H264EncoderImpl::Encode(
     const VideoFrame& input_frame,
     const std::vector<VideoFrameType>* frame_types) {
-  if (encoders_.empty()) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
-  }
-  if (!encoded_image_callback_) {
-    RTC_LOG(LS_WARNING)
-        << "InitEncode() has been called, but a callback function "
-        << "has not been set with RegisterEncodeCompleteCallback()";
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
-  }
-
-  rtc::scoped_refptr<const I420BufferInterface> frame_buffer =
-      input_frame.video_frame_buffer()->ToI420();
-
-  bool send_key_frame = false;
-  for (size_t i = 0; i < configurations_.size(); ++i) {
-    if (configurations_[i].key_frame_request && configurations_[i].sending) {
-      send_key_frame = true;
-      break;
-    }
-  }
-
-  if (!send_key_frame && frame_types) {
-    for (size_t i = 0; i < configurations_.size(); ++i) {
-      const size_t simulcast_idx =
-          static_cast<size_t>(configurations_[i].simulcast_idx);
-      if (configurations_[i].sending && simulcast_idx < frame_types->size() &&
-          (*frame_types)[simulcast_idx] == VideoFrameType::kVideoFrameKey) {
-        send_key_frame = true;
-        break;
-      }
-    }
-  }
-
-  RTC_DCHECK_EQ(configurations_[0].width, frame_buffer->width());
-  RTC_DCHECK_EQ(configurations_[0].height, frame_buffer->height());
-
-  // Encode image for each layer.
-  for (size_t i = 0; i < encoders_.size(); ++i) {
-    // EncodeFrame input.
-    pictures_[i] = {0};
-    pictures_[i].iPicWidth = configurations_[i].width;
-    pictures_[i].iPicHeight = configurations_[i].height;
-    pictures_[i].iColorFormat = EVideoFormatType::videoFormatI420;
-    pictures_[i].uiTimeStamp = input_frame.ntp_time_ms();
-    // Downscale images on second and ongoing layers.
-    if (i == 0) {
-      pictures_[i].iStride[0] = frame_buffer->StrideY();
-      pictures_[i].iStride[1] = frame_buffer->StrideU();
-      pictures_[i].iStride[2] = frame_buffer->StrideV();
-      pictures_[i].pData[0] = const_cast<uint8_t*>(frame_buffer->DataY());
-      pictures_[i].pData[1] = const_cast<uint8_t*>(frame_buffer->DataU());
-      pictures_[i].pData[2] = const_cast<uint8_t*>(frame_buffer->DataV());
-    } else {
-      pictures_[i].iStride[0] = downscaled_buffers_[i - 1]->StrideY();
-      pictures_[i].iStride[1] = downscaled_buffers_[i - 1]->StrideU();
-      pictures_[i].iStride[2] = downscaled_buffers_[i - 1]->StrideV();
-      pictures_[i].pData[0] =
-          const_cast<uint8_t*>(downscaled_buffers_[i - 1]->DataY());
-      pictures_[i].pData[1] =
-          const_cast<uint8_t*>(downscaled_buffers_[i - 1]->DataU());
-      pictures_[i].pData[2] =
-          const_cast<uint8_t*>(downscaled_buffers_[i - 1]->DataV());
-      // Scale the image down a number of times by downsampling factor.
-      libyuv::I420Scale(pictures_[i - 1].pData[0], pictures_[i - 1].iStride[0],
-                        pictures_[i - 1].pData[1], pictures_[i - 1].iStride[1],
-                        pictures_[i - 1].pData[2], pictures_[i - 1].iStride[2],
-                        configurations_[i - 1].width,
-                        configurations_[i - 1].height, pictures_[i].pData[0],
-                        pictures_[i].iStride[0], pictures_[i].pData[1],
-                        pictures_[i].iStride[1], pictures_[i].pData[2],
-                        pictures_[i].iStride[2], configurations_[i].width,
-                        configurations_[i].height, libyuv::kFilterBilinear);
-    }
-
-    if (!configurations_[i].sending) {
-      continue;
-    }
-    if (frame_types != nullptr) {
-      // Skip frame?
-      if ((*frame_types)[i] == VideoFrameType::kEmptyFrame) {
-        continue;
-      }
-    }
-    if (send_key_frame) {
-      // API doc says ForceIntraFrame(false) does nothing, but calling this
-      // function forces a key frame regardless of the |bIDR| argument's value.
-      // (If every frame is a key frame we get lag/delays.)
-      encoders_[i]->ForceIntraFrame(true);
-      configurations_[i].key_frame_request = false;
-    }
-    // EncodeFrame output.
-    SFrameBSInfo info;
-    memset(&info, 0, sizeof(SFrameBSInfo));
-
-    // Encode!
-    int enc_ret = encoders_[i]->EncodeFrame(&pictures_[i], &info);
-    if (enc_ret != 0) {
-      RTC_LOG(LS_ERROR)
-          << "OpenH264 frame encoding failed, EncodeFrame returned " << enc_ret
-          << ".";
-      ReportError();
-      return WEBRTC_VIDEO_CODEC_ERROR;
-    }
-
-    encoded_images_[i]._encodedWidth = configurations_[i].width;
-    encoded_images_[i]._encodedHeight = configurations_[i].height;
-    encoded_images_[i].SetTimestamp(input_frame.timestamp());
-    encoded_images_[i]._frameType = ConvertToVideoFrameType(info.eFrameType);
-    encoded_images_[i].SetSpatialIndex(configurations_[i].simulcast_idx);
-
-    // Split encoded image up into fragments. This also updates
-    // |encoded_image_|.
-    RTPFragmentationHeader frag_header;
-    RtpFragmentize(&encoded_images_[i], *frame_buffer, &info, &frag_header);
-
-    // Encoder can skip frames to save bandwidth in which case
-    // |encoded_images_[i]._length| == 0.
-    if (encoded_images_[i].size() > 0) {
-      // Parse QP.
-      h264_bitstream_parser_.ParseBitstream(encoded_images_[i].data(),
-                                            encoded_images_[i].size());
-      h264_bitstream_parser_.GetLastSliceQp(&encoded_images_[i].qp_);
-
-      // Deliver encoded image.
-      CodecSpecificInfo codec_specific;
-      codec_specific.codecType = kVideoCodecH264;
-      codec_specific.codecSpecific.H264.packetization_mode =
-          packetization_mode_;
-      codec_specific.codecSpecific.H264.temporal_idx = kNoTemporalIdx;
-      codec_specific.codecSpecific.H264.idr_frame =
-          info.eFrameType == videoFrameTypeIDR;
-      codec_specific.codecSpecific.H264.base_layer_sync = false;
-      if (configurations_[i].num_temporal_layers > 1) {
-        const uint8_t tid = info.sLayerInfo[0].uiTemporalId;
-        codec_specific.codecSpecific.H264.temporal_idx = tid;
-        codec_specific.codecSpecific.H264.base_layer_sync =
-            tid > 0 && tid < tl0sync_limit_[i];
-        if (codec_specific.codecSpecific.H264.base_layer_sync) {
-          tl0sync_limit_[i] = tid;
-        }
-        if (tid == 0) {
-          tl0sync_limit_[i] = configurations_[i].num_temporal_layers;
-        }
-      }
-      encoded_image_callback_->OnEncodedImage(encoded_images_[i],
-                                              &codec_specific, &frag_header);
-    }
-  }
+//  if (encoders_.empty()) {
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+//  }
+//  if (!encoded_image_callback_) {
+//    RTC_LOG(LS_WARNING)
+//        << "InitEncode() has been called, but a callback function "
+//        << "has not been set with RegisterEncodeCompleteCallback()";
+//    ReportError();
+//    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+//  }
+//
+//  rtc::scoped_refptr<const I420BufferInterface> frame_buffer =
+//      input_frame.video_frame_buffer()->ToI420();
+//
+//  bool send_key_frame = false;
+//  for (size_t i = 0; i < configurations_.size(); ++i) {
+//    if (configurations_[i].key_frame_request && configurations_[i].sending) {
+//      send_key_frame = true;
+//      break;
+//    }
+//  }
+//
+//  if (!send_key_frame && frame_types) {
+//    for (size_t i = 0; i < configurations_.size(); ++i) {
+//      const size_t simulcast_idx =
+//          static_cast<size_t>(configurations_[i].simulcast_idx);
+//      if (configurations_[i].sending && simulcast_idx < frame_types->size() &&
+//          (*frame_types)[simulcast_idx] == VideoFrameType::kVideoFrameKey) {
+//        send_key_frame = true;
+//        break;
+//      }
+//    }
+//  }
+//
+//  RTC_DCHECK_EQ(configurations_[0].width, frame_buffer->width());
+//  RTC_DCHECK_EQ(configurations_[0].height, frame_buffer->height());
+//
+//  // Encode image for each layer.
+//  for (size_t i = 0; i < encoders_.size(); ++i) {
+//    // EncodeFrame input.
+//    pictures_[i] = {0};
+//    pictures_[i].iPicWidth = configurations_[i].width;
+//    pictures_[i].iPicHeight = configurations_[i].height;
+//    pictures_[i].iColorFormat = EVideoFormatType::videoFormatI420;
+//    pictures_[i].uiTimeStamp = input_frame.ntp_time_ms();
+//    // Downscale images on second and ongoing layers.
+//    if (i == 0) {
+//      pictures_[i].iStride[0] = frame_buffer->StrideY();
+//      pictures_[i].iStride[1] = frame_buffer->StrideU();
+//      pictures_[i].iStride[2] = frame_buffer->StrideV();
+//      pictures_[i].pData[0] = const_cast<uint8_t*>(frame_buffer->DataY());
+//      pictures_[i].pData[1] = const_cast<uint8_t*>(frame_buffer->DataU());
+//      pictures_[i].pData[2] = const_cast<uint8_t*>(frame_buffer->DataV());
+//    } else {
+//      pictures_[i].iStride[0] = downscaled_buffers_[i - 1]->StrideY();
+//      pictures_[i].iStride[1] = downscaled_buffers_[i - 1]->StrideU();
+//      pictures_[i].iStride[2] = downscaled_buffers_[i - 1]->StrideV();
+//      pictures_[i].pData[0] =
+//          const_cast<uint8_t*>(downscaled_buffers_[i - 1]->DataY());
+//      pictures_[i].pData[1] =
+//          const_cast<uint8_t*>(downscaled_buffers_[i - 1]->DataU());
+//      pictures_[i].pData[2] =
+//          const_cast<uint8_t*>(downscaled_buffers_[i - 1]->DataV());
+//      // Scale the image down a number of times by downsampling factor.
+//      libyuv::I420Scale(pictures_[i - 1].pData[0], pictures_[i - 1].iStride[0],
+//                        pictures_[i - 1].pData[1], pictures_[i - 1].iStride[1],
+//                        pictures_[i - 1].pData[2], pictures_[i - 1].iStride[2],
+//                        configurations_[i - 1].width,
+//                        configurations_[i - 1].height, pictures_[i].pData[0],
+//                        pictures_[i].iStride[0], pictures_[i].pData[1],
+//                        pictures_[i].iStride[1], pictures_[i].pData[2],
+//                        pictures_[i].iStride[2], configurations_[i].width,
+//                        configurations_[i].height, libyuv::kFilterBilinear);
+//    }
+//
+//    if (!configurations_[i].sending) {
+//      continue;
+//    }
+//    if (frame_types != nullptr) {
+//      // Skip frame?
+//      if ((*frame_types)[i] == VideoFrameType::kEmptyFrame) {
+//        continue;
+//      }
+//    }
+//    if (send_key_frame) {
+//      // API doc says ForceIntraFrame(false) does nothing, but calling this
+//      // function forces a key frame regardless of the |bIDR| argument's value.
+//      // (If every frame is a key frame we get lag/delays.)
+//      encoders_[i]->ForceIntraFrame(true);
+//      configurations_[i].key_frame_request = false;
+//    }
+//    // EncodeFrame output.
+//    SFrameBSInfo info;
+//    memset(&info, 0, sizeof(SFrameBSInfo));
+//
+//    // Encode!
+//    int enc_ret = encoders_[i]->EncodeFrame(&pictures_[i], &info);
+//    if (enc_ret != 0) {
+//      RTC_LOG(LS_ERROR)
+//          << "OpenH264 frame encoding failed, EncodeFrame returned " << enc_ret
+//          << ".";
+//      ReportError();
+//      return WEBRTC_VIDEO_CODEC_ERROR;
+//    }
+//
+//    encoded_images_[i]._encodedWidth = configurations_[i].width;
+//    encoded_images_[i]._encodedHeight = configurations_[i].height;
+//    encoded_images_[i].SetTimestamp(input_frame.timestamp());
+//    encoded_images_[i]._frameType = ConvertToVideoFrameType(info.eFrameType);
+//    encoded_images_[i].SetSpatialIndex(configurations_[i].simulcast_idx);
+//
+//    // Split encoded image up into fragments. This also updates
+//    // |encoded_image_|.
+//    RTPFragmentationHeader frag_header;
+//    RtpFragmentize(&encoded_images_[i], *frame_buffer, &info, &frag_header);
+//
+//    // Encoder can skip frames to save bandwidth in which case
+//    // |encoded_images_[i]._length| == 0.
+//    if (encoded_images_[i].size() > 0) {
+//      // Parse QP.
+//      h264_bitstream_parser_.ParseBitstream(encoded_images_[i].data(),
+//                                            encoded_images_[i].size());
+//      h264_bitstream_parser_.GetLastSliceQp(&encoded_images_[i].qp_);
+//
+//      // Deliver encoded image.
+//      CodecSpecificInfo codec_specific;
+//      codec_specific.codecType = kVideoCodecH264;
+//      codec_specific.codecSpecific.H264.packetization_mode =
+//          packetization_mode_;
+//      codec_specific.codecSpecific.H264.temporal_idx = kNoTemporalIdx;
+//      codec_specific.codecSpecific.H264.idr_frame =
+//          info.eFrameType == videoFrameTypeIDR;
+//      codec_specific.codecSpecific.H264.base_layer_sync = false;
+//      if (configurations_[i].num_temporal_layers > 1) {
+//        const uint8_t tid = info.sLayerInfo[0].uiTemporalId;
+//        codec_specific.codecSpecific.H264.temporal_idx = tid;
+//        codec_specific.codecSpecific.H264.base_layer_sync =
+//            tid > 0 && tid < tl0sync_limit_[i];
+//        if (codec_specific.codecSpecific.H264.base_layer_sync) {
+//          tl0sync_limit_[i] = tid;
+//        }
+//        if (tid == 0) {
+//          tl0sync_limit_[i] = configurations_[i].num_temporal_layers;
+//        }
+//      }
+//      encoded_image_callback_->OnEncodedImage(encoded_images_[i],
+//                                              &codec_specific, &frag_header);
+//    }
+//  }
   return WEBRTC_VIDEO_CODEC_OK;
 }
 
@@ -530,74 +530,74 @@ int32_t H264EncoderImpl::Encode(
 // memset(&p, 0, sizeof(SEncParamBase)) used in Initialize, and SEncParamExt
 // which is a superset of SEncParamBase (cleared with GetDefaultParams) used
 // in InitializeExt.
-SEncParamExt H264EncoderImpl::CreateEncoderParams(size_t i) const {
-  SEncParamExt encoder_params;
-  encoders_[i]->GetDefaultParams(&encoder_params);
-  if (codec_.mode == VideoCodecMode::kRealtimeVideo) {
-    encoder_params.iUsageType = CAMERA_VIDEO_REAL_TIME;
-  } else if (codec_.mode == VideoCodecMode::kScreensharing) {
-    encoder_params.iUsageType = SCREEN_CONTENT_REAL_TIME;
-  } else {
-    RTC_NOTREACHED();
-  }
-  encoder_params.iPicWidth = configurations_[i].width;
-  encoder_params.iPicHeight = configurations_[i].height;
-  encoder_params.iTargetBitrate = configurations_[i].target_bps;
-  encoder_params.iMaxBitrate = configurations_[i].max_bps;
-  // Rate Control mode
-  encoder_params.iRCMode = RC_BITRATE_MODE;
-  encoder_params.fMaxFrameRate = configurations_[i].max_frame_rate;
-
-  // The following parameters are extension parameters (they're in SEncParamExt,
-  // not in SEncParamBase).
-  encoder_params.bEnableFrameSkip = configurations_[i].frame_dropping_on;
-  // |uiIntraPeriod|    - multiple of GOP size
-  // |keyFrameInterval| - number of frames
-  encoder_params.uiIntraPeriod = configurations_[i].key_frame_interval;
-  encoder_params.uiMaxNalSize = 0;
-  // Threading model: use auto.
-  //  0: auto (dynamic imp. internal encoder)
-  //  1: single thread (default value)
-  // >1: number of threads
-  encoder_params.iMultipleThreadIdc = NumberOfThreads(
-      encoder_params.iPicWidth, encoder_params.iPicHeight, number_of_cores_);
-  // The base spatial layer 0 is the only one we use.
-  encoder_params.sSpatialLayers[0].iVideoWidth = encoder_params.iPicWidth;
-  encoder_params.sSpatialLayers[0].iVideoHeight = encoder_params.iPicHeight;
-  encoder_params.sSpatialLayers[0].fFrameRate = encoder_params.fMaxFrameRate;
-  encoder_params.sSpatialLayers[0].iSpatialBitrate =
-      encoder_params.iTargetBitrate;
-  encoder_params.sSpatialLayers[0].iMaxSpatialBitrate =
-      encoder_params.iMaxBitrate;
-  encoder_params.iTemporalLayerNum = configurations_[i].num_temporal_layers;
-  if (encoder_params.iTemporalLayerNum > 1) {
-    encoder_params.iNumRefFrame = 1;
-  }
-  RTC_LOG(INFO) << "OpenH264 version is " << OPENH264_MAJOR << "."
-                << OPENH264_MINOR;
-  switch (packetization_mode_) {
-    case H264PacketizationMode::SingleNalUnit:
-      // Limit the size of the packets produced.
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
-          SM_SIZELIMITED_SLICE;
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceSizeConstraint =
-          static_cast<unsigned int>(max_payload_size_);
-      RTC_LOG(INFO) << "Encoder is configured with NALU constraint: "
-                    << max_payload_size_ << " bytes";
-      break;
-    case H264PacketizationMode::NonInterleaved:
-      // When uiSliceMode = SM_FIXEDSLCNUM_SLICE, uiSliceNum = 0 means auto
-      // design it with cpu core number.
-      // TODO(sprang): Set to 0 when we understand why the rate controller borks
-      //               when uiSliceNum > 1.
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
-          SM_FIXEDSLCNUM_SLICE;
-      break;
-  }
-  return encoder_params;
-}
+//SEncParamExt H264EncoderImpl::CreateEncoderParams(size_t i) const {
+//  SEncParamExt encoder_params;
+//  encoders_[i]->GetDefaultParams(&encoder_params);
+//  if (codec_.mode == VideoCodecMode::kRealtimeVideo) {
+//    encoder_params.iUsageType = CAMERA_VIDEO_REAL_TIME;
+//  } else if (codec_.mode == VideoCodecMode::kScreensharing) {
+//    encoder_params.iUsageType = SCREEN_CONTENT_REAL_TIME;
+//  } else {
+//    RTC_NOTREACHED();
+//  }
+//  encoder_params.iPicWidth = configurations_[i].width;
+//  encoder_params.iPicHeight = configurations_[i].height;
+//  encoder_params.iTargetBitrate = configurations_[i].target_bps;
+//  encoder_params.iMaxBitrate = configurations_[i].max_bps;
+//  // Rate Control mode
+//  encoder_params.iRCMode = RC_BITRATE_MODE;
+//  encoder_params.fMaxFrameRate = configurations_[i].max_frame_rate;
+//
+//  // The following parameters are extension parameters (they're in SEncParamExt,
+//  // not in SEncParamBase).
+//  encoder_params.bEnableFrameSkip = configurations_[i].frame_dropping_on;
+//  // |uiIntraPeriod|    - multiple of GOP size
+//  // |keyFrameInterval| - number of frames
+//  encoder_params.uiIntraPeriod = configurations_[i].key_frame_interval;
+//  encoder_params.uiMaxNalSize = 0;
+//  // Threading model: use auto.
+//  //  0: auto (dynamic imp. internal encoder)
+//  //  1: single thread (default value)
+//  // >1: number of threads
+//  encoder_params.iMultipleThreadIdc = NumberOfThreads(
+//      encoder_params.iPicWidth, encoder_params.iPicHeight, number_of_cores_);
+//  // The base spatial layer 0 is the only one we use.
+//  encoder_params.sSpatialLayers[0].iVideoWidth = encoder_params.iPicWidth;
+//  encoder_params.sSpatialLayers[0].iVideoHeight = encoder_params.iPicHeight;
+//  encoder_params.sSpatialLayers[0].fFrameRate = encoder_params.fMaxFrameRate;
+//  encoder_params.sSpatialLayers[0].iSpatialBitrate =
+//      encoder_params.iTargetBitrate;
+//  encoder_params.sSpatialLayers[0].iMaxSpatialBitrate =
+//      encoder_params.iMaxBitrate;
+//  encoder_params.iTemporalLayerNum = configurations_[i].num_temporal_layers;
+//  if (encoder_params.iTemporalLayerNum > 1) {
+//    encoder_params.iNumRefFrame = 1;
+//  }
+//  RTC_LOG(INFO) << "OpenH264 version is " << OPENH264_MAJOR << "."
+//                << OPENH264_MINOR;
+//  switch (packetization_mode_) {
+//    case H264PacketizationMode::SingleNalUnit:
+//      // Limit the size of the packets produced.
+//      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
+//      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
+//          SM_SIZELIMITED_SLICE;
+//      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceSizeConstraint =
+//          static_cast<unsigned int>(max_payload_size_);
+//      RTC_LOG(INFO) << "Encoder is configured with NALU constraint: "
+//                    << max_payload_size_ << " bytes";
+//      break;
+//    case H264PacketizationMode::NonInterleaved:
+//      // When uiSliceMode = SM_FIXEDSLCNUM_SLICE, uiSliceNum = 0 means auto
+//      // design it with cpu core number.
+//      // TODO(sprang): Set to 0 when we understand why the rate controller borks
+//      //               when uiSliceNum > 1.
+//      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
+//      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
+//          SM_FIXEDSLCNUM_SLICE;
+//      break;
+//  }
+//  return encoder_params;
+//}
 
 void H264EncoderImpl::ReportInit() {
   if (has_reported_init_)
diff --git a/modules/video_coding/codecs/h264/h264_encoder_impl.h b/modules/video_coding/codecs/h264/h264_encoder_impl.h
index ba996366a3..6545668059 100644
--- a/modules/video_coding/codecs/h264/h264_encoder_impl.h
+++ b/modules/video_coding/codecs/h264/h264_encoder_impl.h
@@ -17,9 +17,9 @@
 // #ifdef unless needed and tested.
 #ifdef WEBRTC_USE_H264
 
-#if defined(WEBRTC_WIN) && !defined(__clang__)
-#error "See: bugs.webrtc.org/9213#c13."
-#endif
+//#if defined(WEBRTC_WIN) && !defined(__clang__)
+//#error "See: bugs.webrtc.org/9213#c13."
+//#endif
 
 #include <memory>
 #include <vector>
@@ -29,7 +29,7 @@
 #include "common_video/h264/h264_bitstream_parser.h"
 #include "modules/video_coding/codecs/h264/include/h264.h"
 #include "modules/video_coding/utility/quality_scaler.h"
-#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
+//#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
 
 class ISVCEncoder;
 
@@ -85,15 +85,15 @@ class H264EncoderImpl : public H264Encoder {
   }
 
  private:
-  SEncParamExt CreateEncoderParams(size_t i) const;
+  //SEncParamExt CreateEncoderParams(size_t i) const;
 
   webrtc::H264BitstreamParser h264_bitstream_parser_;
   // Reports statistics with histograms.
   void ReportInit();
   void ReportError();
 
-  std::vector<ISVCEncoder*> encoders_;
-  std::vector<SSourcePicture> pictures_;
+  //std::vector<ISVCEncoder*> encoders_;
+  //std::vector<SSourcePicture> pictures_;
   std::vector<rtc::scoped_refptr<I420Buffer>> downscaled_buffers_;
   std::vector<LayerConfig> configurations_;
   std::vector<EncodedImage> encoded_images_;
-- 
2.22.0.vfs.1.1.57.gbaf16c8

